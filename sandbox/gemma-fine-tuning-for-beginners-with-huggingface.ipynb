{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Gemma LoRA Fine-tuning for Beginners with Hugging Face\n",
    "\n",
    "In this notebook, we'll learn the very basics of using the Gemma model, incorporating the powerful tools from Hugging Face. It's focused on the simplest content without any complex processing. This practical exercise is about training a Large Language Model (LLM) to generate Python Q&A using the Gemma model with the support of Hugging Face libraries.\n",
    "\n",
    "### Table of Contents:\n",
    " \n",
    "1. What is Gemma?<br>\n",
    "2. Package Installation and Importing<br>\n",
    "3. Data Loading <br>\n",
    "4. Data Preprocessing for Training<br>\n",
    "5. Loading the Gemma Model<br>\n",
    "7. Q & A Results Before Finetuning<br>\n",
    "7. Applying Gemma LoRA<br>\n",
    "8. Training Gemma<br>\n",
    "9. Q & A Results After Finetuning<br>\n",
    "10. Conclusion<br>\n",
    "\n",
    "### Dataset Used\n",
    "- [Dataset_Python_Question_Answer](https://www.kaggle.com/datasets/chinmayadatt/dataset-python-question-answer) : This dataset is about Python programming. Question and answers are generated using Gemma. There are more than four hundred questions and their corresponding answers about Python programming.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.What is Gemma?\n",
    "\n",
    "Gemma is a powerful machine learning model designed for a wide range of tasks. This section will introduce the basics of Gemma, its use cases, and why it's beneficial for your projects.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- Gemma models are built from the ground up to be lightweight and state-of-the-art. They are text-to-text, decoder-only large language models, available primarily in English.\n",
    "- They come with open weights, offering both pre-trained and instruction-tuned variants to suit a wide array of text generation tasks.\n",
    "- Ideal for applications such as question answering, summarization, and reasoning, Gemma models can be deployed on relatively modest hardware, including laptops and desktops, or within your own cloud infrastructure.\n",
    "\n",
    "### Description\n",
    "\n",
    "- **Lightweight and Open**: Gemma models are designed to be both powerful and accessible, embodying Google's commitment to democratizing state-of-the-art AI technology.\n",
    "- **Versatile Applications**: Whether it's generating answers to questions, summarizing documents, or facilitating complex reasoning tasks, Gemma models are equipped to handle a diverse set of challenges.\n",
    "- **Democratizing AI**: By making Gemma models lightweight and open, Google ensures that cutting-edge AI technology is no longer confined to those with access to extensive computational resources.\n",
    "\n",
    "### Inputs and Outputs\n",
    "\n",
    "- **Input**: Gemma models take in text strings, which can range from questions and prompts to longer documents that require summarization.\n",
    "- **Output**: In response, they generate text in English, offering answers, summaries, or other forms of text-based output, tailored to the input provided.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Package Installation and Importing\n",
    "\n",
    "Before we start, it's essential to install all necessary packages, including Gemma itself. This part will cover the installation process step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Description\n",
    "\n",
    "#### python basic module\n",
    "- `os`: Provides ways to interact with the operating system and its environment variables.\n",
    "- `torch`: PyTorch library for deep learning applications.\n",
    "- `numpy`: Essential library for linear algebra and mathematical operations.\n",
    "- `pandas`: Powerful data processing tool, ideal for handling CSV files and other forms of structured data.\n",
    "\n",
    "#### transformers module\n",
    "- `AutoTokenizer`: Used to automatically load a pre-trained tokenizer.\n",
    "- `AutoModelForCausalLM`: Used to automatically load pre-trained models for causal language modeling.\n",
    "- `BitsAndBytesConfig`: Configuration class for setting up the Bits and Bytes tokenizer.\n",
    "- `AutoConfig`: Used to automatically load the model's configuration.\n",
    "- `TrainingArguments`: Defines arguments for training setup.\n",
    "\n",
    "#### datasets module\n",
    "- `Dataset`: A class for handling datasets.\n",
    "\n",
    "#### peft module\n",
    "- `LoraConfig`: A configuration class for configuring the Lora model.\n",
    "- `PeftModel`: A class that defines the PEFT model.\n",
    "- `prepare_model_for_kbit_training`: A function that prepares a model for k-bit training.\n",
    "- `get_peft_model`: Function to get the PEFT model.\n",
    "\n",
    "#### trl module\n",
    "- `SFTTrainer`: Trainer class for SFT (Supervised Fine-Tuning) training.\n",
    "\n",
    "#### IPython.display module\n",
    "- `Markdown`: Used to output text in Markdown format.\n",
    "- `display`: Used to display objects in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T17:18:48.416472Z",
     "iopub.status.busy": "2024-04-13T17:18:48.415542Z",
     "iopub.status.idle": "2024-04-13T17:18:48.423236Z",
     "shell.execute_reply": "2024-04-13T17:18:48.422147Z",
     "shell.execute_reply.started": "2024-04-13T17:18:48.416430Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\ByteMentor\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForCausalLM, \n",
    "                          BitsAndBytesConfig, \n",
    "                          AutoConfig,\n",
    "                          TrainingArguments\n",
    "                          )\n",
    "\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T17:18:48.424852Z",
     "iopub.status.busy": "2024-04-13T17:18:48.424509Z",
     "iopub.status.idle": "2024-04-13T17:18:48.435321Z",
     "shell.execute_reply": "2024-04-13T17:18:48.434352Z",
     "shell.execute_reply.started": "2024-04-13T17:18:48.424820Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "# Disable CA bundle check. Useful in certain environments where you may encounter SSL errors.\n",
    "import torch\n",
    "\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "\n",
    "# Set the order of devices as seen by CUDA to PCI bus ID order. This is to ensure consistency in device selection.\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "# Check if CUDA is available, and if so, specify which GPU(s) to be made visible to the process.\n",
    "if torch.cuda.is_available():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Only set this if CUDA is available.\n",
    "    print(\"CUDA is available\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tool for tracking and visualizing Machine Learning experiments. Wandb helps you easily manage metrics, hyperparameters, experiment code, and model artifacts during model training.<br>\n",
    "<a href=\"https://github.com/wandb/wandb\">wandb github</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Loading\n",
    "\n",
    "Loading your data is the first step in the machine learning pipeline. This section will guide you through loading your dataset into the Jupyter notebook environment.\n",
    "\n",
    "### To download a dataset, follow these simple steps:\n",
    "1. Look for the \"Input\" option located below the \"Notebook\" section in the right-side menu.\n",
    "2. Click on the \"+ Add Input\" button.\n",
    "3. In the search bar that appears, type \"dataset-python-question-answer\".\n",
    "4. Find the dataset in the search results and click the \"+\" button to add it to your notebook. This action will automatically download the dataset for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T17:18:48.469811Z",
     "iopub.status.busy": "2024-04-13T17:18:48.469452Z",
     "iopub.status.idle": "2024-04-13T17:18:48.482294Z",
     "shell.execute_reply": "2024-04-13T17:18:48.481337Z",
     "shell.execute_reply.started": "2024-04-13T17:18:48.469769Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize a variable to hold the full path to the target CSV file.\n",
    "csv_file_path = 'D:\\\\Projects\\\\ByteMentor\\\\data\\\\data_training.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T17:18:48.483737Z",
     "iopub.status.busy": "2024-04-13T17:18:48.483477Z",
     "iopub.status.idle": "2024-04-13T17:18:48.507680Z",
     "shell.execute_reply": "2024-04-13T17:18:48.506729Z",
     "shell.execute_reply.started": "2024-04-13T17:18:48.483714Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_data shape: (34783, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from the identified CSV file.\n",
    "# csv_file_path = \"/kaggle/input/dataset-python-question-answer/Dataset_Python_Question_Answer.csv\"\n",
    "original_data = pd.read_csv(csv_file_path, encoding='utf-8', sep=';', header=0)\n",
    "\n",
    "# Print the shape of the dataset to understand its dimensions (number of rows and columns).\n",
    "print('original_data shape:',original_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T17:18:48.509047Z",
     "iopub.status.busy": "2024-04-13T17:18:48.508766Z",
     "iopub.status.idle": "2024-04-13T17:18:48.519507Z",
     "shell.execute_reply": "2024-04-13T17:18:48.518592Z",
     "shell.execute_reply.started": "2024-04-13T17:18:48.509022Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30079</th>\n",
       "      <td>What is the fundamental concept of data proces...</td>\n",
       "      <td>Data processing involves transforming raw data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8207</th>\n",
       "      <td>What is the role of data transformation in Dat...</td>\n",
       "      <td>Data transformation prepares data for effectiv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Question  \\\n",
       "30079  What is the fundamental concept of data proces...   \n",
       "8207   What is the role of data transformation in Dat...   \n",
       "\n",
       "                                                  Answer  \n",
       "30079  Data processing involves transforming raw data...  \n",
       "8207   Data transformation prepares data for effectiv...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a random sample of 2 rows from the original_data to get a quick overview of the data.\n",
    "original_data.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Preprocessing for Training\n",
    "\n",
    "Before initiating the training process with Google's Gemma, a pivotal step involves the preparation of our dataset. The core of this stage is to align our dataset with the specifications required by Gemma, ensuring optimal compatibility and efficiency in training. The process commences with the strategic manipulation of our dataset, specifically focusing on the 'Question' and 'Answer' columns. These columns are instrumental as we meticulously combine them to form comprehensive training examples, thereby facilitating a seamless training experience.\n",
    "\n",
    "A critical aspect to acknowledge during data preprocessing is the management of data length. Given that the Gemma model operates as a Large Language Model (LLM), it's imperative to assess the length of our training data. Training with excessively lengthy data could impose substantial demands on GPU resources, potentially hindering the efficiency of the process. To circumvent this challenge and optimize resource utilization, we advocate for the exclusion of unduly long data from the training set. This strategic decision not only preserves GPU resources but also ensures a more streamlined and effective training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T17:18:48.521560Z",
     "iopub.status.busy": "2024-04-13T17:18:48.521215Z",
     "iopub.status.idle": "2024-04-13T17:18:48.535154Z",
     "shell.execute_reply": "2024-04-13T17:18:48.534281Z",
     "shell.execute_reply.started": "2024-04-13T17:18:48.521525Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of 'Question and Answer' in original dataset: 153\n",
      "Shortest length of 'Question and Answer' in original dataset: 57\n",
      "Longest length of 'Question and Answer' in original dataset: 771\n"
     ]
    }
   ],
   "source": [
    "question_column = \"Question\"\n",
    "answer_column = \"Answer\"\n",
    "\n",
    "# Calculate the length of each 'Question' and 'Answer' combined and add it as a new column\n",
    "original_data['text_length'] = original_data[question_column].str.len() + original_data[answer_column].str.len()\n",
    "\n",
    "# Calculate the average length of 'Answer' in the filtered dataset\n",
    "average_length = int(original_data['text_length'].mean())\n",
    "\n",
    "# Find the shortest and longest lengths of 'Answer' in the filtered dataset\n",
    "shortest_length = int(original_data['text_length'].min())\n",
    "longest_length = int(original_data['text_length'].max())\n",
    "\n",
    "# Print the statistics\n",
    "print(\"Average length of 'Question and Answer' in original dataset:\", average_length)\n",
    "print(\"Shortest length of 'Question and Answer' in original dataset:\", shortest_length)\n",
    "print(\"Longest length of 'Question and Answer' in original dataset:\", longest_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T17:18:48.536760Z",
     "iopub.status.busy": "2024-04-13T17:18:48.536471Z",
     "iopub.status.idle": "2024-04-13T17:18:48.548774Z",
     "shell.execute_reply": "2024-04-13T17:18:48.547920Z",
     "shell.execute_reply.started": "2024-04-13T17:18:48.536737Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries before filtering: 34783\n",
      "Number of entries after filtering: 17709\n",
      "------------------------------\n",
      "Maximum text length before filtering: 771\n",
      "Maximum text length after filtering: 151\n"
     ]
    }
   ],
   "source": [
    "# Calculate the median length of 'text_length' to set a threshold for filtering\n",
    "median_text_length_threshold = int(original_data['text_length'].quantile(0.5))\n",
    "\n",
    "# Retain only rows where 'text_length' is less than or equal to the median text length\n",
    "filtered_data = original_data[original_data['text_length'] <= median_text_length_threshold]\n",
    "\n",
    "# Output the number of entries before and after filtering to assess the impact\n",
    "print(\"Number of entries before filtering:\", len(original_data))\n",
    "print(\"Number of entries after filtering:\", len(filtered_data))\n",
    "\n",
    "print(\"---\"*10)\n",
    "\n",
    "# Determine the maximum 'text_length' in the filtered dataset\n",
    "max_text_length_in_filtered_data = int(filtered_data['text_length'].max())\n",
    "\n",
    "# Compare the maximum 'text_length' before and after filtering\n",
    "print(f\"Maximum text length before filtering: {longest_length}\\nMaximum text length after filtering: {max_text_length_in_filtered_data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T17:18:48.552445Z",
     "iopub.status.busy": "2024-04-13T17:18:48.552092Z",
     "iopub.status.idle": "2024-04-13T17:18:48.563742Z",
     "shell.execute_reply": "2024-04-13T17:18:48.562819Z",
     "shell.execute_reply.started": "2024-04-13T17:18:48.552420Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24663</th>\n",
       "      <td>Lineare_Algebra deals with vector spaces.</td>\n",
       "      <td>Vector spaces are fundamental in Lineare_Algebra.</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5483</th>\n",
       "      <td>What is data integrity, and why is it important?</td>\n",
       "      <td>Data integrity ensures data accuracy and relia...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Question  \\\n",
       "24663         Lineare_Algebra deals with vector spaces.   \n",
       "5483   What is data integrity, and why is it important?   \n",
       "\n",
       "                                                  Answer  text_length  \n",
       "24663  Vector spaces are fundamental in Lineare_Algebra.           90  \n",
       "5483   Data integrity ensures data accuracy and relia...          140  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a random sample of 2 rows from the filtered_data to get a quick overview of the data.\n",
    "filtered_data.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, it's **essential** to highlight the integration with the Hugging Face's transformers library, a pivotal component in our data preprocessing journey. This integration necessitates the conversion of our dataset into a specific format, namely `from datasets import Dataset`. This adjustment is crucial as it aligns with the library's requirements, enabling us to leverage its full potential in facilitating the training of the Gemma model. By adhering to this format, we ensure a harmonious and efficient interaction with the transformers library, further enhancing the overall training process.\n",
    "<a href=\"https://huggingface.co/docs/transformers/index\">Transformers documentation</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T17:18:48.565449Z",
     "iopub.status.busy": "2024-04-13T17:18:48.565048Z",
     "iopub.status.idle": "2024-04-13T17:18:48.586725Z",
     "shell.execute_reply": "2024-04-13T17:18:48.585559Z",
     "shell.execute_reply.started": "2024-04-13T17:18:48.565418Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Data structure>\n",
      "Dataset({\n",
      "    features: ['Question', 'Answer', 'text_length', '__index_level_0__'],\n",
      "    num_rows: 17709\n",
      "})\n",
      "\n",
      "\n",
      "<Random sample dataset>\n",
      "\n",
      "- Question: What is the fundamental concept behind data product creation?\n",
      "\n",
      "- Answer: Data product creation builds value from data, leveraging analytical insights.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Convert dataset to Dataset object\n",
    "dataset = Dataset.from_pandas(filtered_data)\n",
    "\n",
    "# Print the entire dataset\n",
    "print(\"<Data structure>\")\n",
    "print(dataset)\n",
    "\n",
    "# Generate a random index based on the dataset length\n",
    "random_index = random.randint(0, len(dataset) - 1)\n",
    "\n",
    "# Print a random sample of the dataset\n",
    "print(\"\\n\\n<Random sample dataset>\")\n",
    "print(\"\\n- Question:\", dataset[random_index][question_column])\n",
    "print(\"\\n- Answer:\", dataset[random_index][answer_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Loading the Gemma Model\n",
    "\n",
    "Here, we'll cover how to load the Gemma model so it's ready for finetuning. This includes where to download the model from and how to load it into your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Gemma Model\n",
    "1. Still in the \"Input\" section of the right-side menu in your Kaggle notebook, click on the \"+ Add Input\" button again.\n",
    "2. Below the search bar that appears, click on the \"Models\" option.\n",
    "3. In the search bar, type \"Gemma\" to find the model.\n",
    "4. From the filtered results, select the Gemma model by clicking on the \"+\" button next to it. Make sure to choose the correct version by noting the framework as \"Transformers\", the variation as \"2b-it\", and the version as \"v3\".\n",
    "5. After selecting the correct Gemma model, click on \"Add Model\" at the bottom.\n",
    "6. The Gemma model, specifically \"Gemma.v3\", should now be listed under the \"Models\" subsection of the \"Input\" section in the right-side menu of your notebook, indicating successful addition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BitsAndBytesConfig Overview\n",
    "\n",
    "`BitsAndBytesConfig` is a configuration class provided by the `transformers` library, which is designed for controlling the behavior of model quantization and optimization during both the training and inference phases of model deployment. Quantization is a technique used to reduce the memory footprint and computational requirements of deep learning models by representing model weights and activations in lower-precision data types, such as 8-bit integers (`int8`) or even 4-bit representations.\n",
    "\n",
    "#### Benefits of Quantization\n",
    "\n",
    "The primary benefits of quantization include:\n",
    "\n",
    "- **Reduced Memory Usage**: Lower-precision representations require less memory, enabling the deployment of larger models on devices with limited memory capacity.\n",
    "- **Increased Inference Speed**: Operations with lower-precision data types can be executed faster, thus speeding up the inference time.\n",
    "- **Energy Efficiency**: Reduced computational requirements translate to lower energy consumption, which is crucial for mobile and embedded devices.\n",
    "\n",
    "#### `BitsAndBytesConfig` Parameters\n",
    "\n",
    "In the context of the `transformers` library, `BitsAndBytesConfig` allows users to configure the quantization behavior specifically for using the `bitsandbytes` backend. Below is an example configuration along with comments explaining each parameter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T17:18:48.599133Z",
     "iopub.status.busy": "2024-04-13T17:18:48.598282Z",
     "iopub.status.idle": "2024-04-13T17:18:55.870752Z",
     "shell.execute_reply": "2024-04-13T17:18:55.869804Z",
     "shell.execute_reply.started": "2024-04-13T17:18:48.599089Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\ByteMentor\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"google/gemma-2b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T17:18:55.872415Z",
     "iopub.status.busy": "2024-04-13T17:18:55.872024Z",
     "iopub.status.idle": "2024-04-13T17:18:55.881031Z",
     "shell.execute_reply": "2024-04-13T17:18:55.880177Z",
     "shell.execute_reply.started": "2024-04-13T17:18:55.872382Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a summary of the model to understand its architecture and the number of parameters.\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting generating Text with the Gemma Model\n",
    "\n",
    "This code provides a simple function to generate text using the Gemma model. The Gemma model, a variant of large language models, excels in generating human-like text based on a given prompt. This function utilizes both a model and tokenizer from the Gemma architecture, formatting the output in a specific template for clarity and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T17:18:55.883004Z",
     "iopub.status.busy": "2024-04-13T17:18:55.882371Z",
     "iopub.status.idle": "2024-04-13T17:18:55.891999Z",
     "shell.execute_reply": "2024-04-13T17:18:55.891110Z",
     "shell.execute_reply.started": "2024-04-13T17:18:55.882969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define a template for formatting instructions and responses.\n",
    "# This template will be used to format the text data in a LLM structure.\n",
    "template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T17:18:55.893501Z",
     "iopub.status.busy": "2024-04-13T17:18:55.893206Z",
     "iopub.status.idle": "2024-04-13T17:18:55.904199Z",
     "shell.execute_reply": "2024-04-13T17:18:55.903396Z",
     "shell.execute_reply.started": "2024-04-13T17:18:55.893477Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, device, max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    This function generates a response to a given prompt using a specified model and tokenizer.\n",
    "\n",
    "    Parameters:\n",
    "    - model (PreTrainedModel): The machine learning model pre-trained for text generation.\n",
    "    - tokenizer (PreTrainedTokenizer): A tokenizer for converting text into a format the model understands.\n",
    "    - prompt (str): The initial text prompt to generate a response for.\n",
    "    - device (torch.device): The computing device (CPU or GPU) the model should use for calculations.\n",
    "    - max_new_tokens (int, optional): The maximum number of new tokens to generate. Defaults to 128.\n",
    "\n",
    "    Returns:\n",
    "    - str: The text generated in response to the prompt.\n",
    "    \"\"\"\n",
    "    # Convert the prompt into a format the model can understand using the tokenizer.\n",
    "    # The result is also moved to the specified computing device.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Generate a response based on the tokenized prompt.\n",
    "    outputs = model.generate(**inputs, num_return_sequences=1, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Convert the generated tokens back into readable text.\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract and return the response text. Here, it assumes the response is formatted as \"Response: [generated text]\".\n",
    "    response_text = text.split(\"Response:\")[1]\n",
    "    \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Q & A Results Before Finetuning\n",
    "\n",
    "Before we start the finetuning process, let's see how the Gemma model performs out of the box on our dataset. This section will show you how to run a simple question-answering test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T17:18:55.905530Z",
     "iopub.status.busy": "2024-04-13T17:18:55.905250Z",
     "iopub.status.idle": "2024-04-13T17:19:02.044786Z",
     "shell.execute_reply": "2024-04-13T17:19:02.043783Z",
     "shell.execute_reply.started": "2024-04-13T17:18:55.905507Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "Sure. Here's the difference between an expression and an operator:\n",
       "\n",
       "**Expression:**\n",
       "\n",
       "* An expression is a combination of numbers, variables, operators, and literals that is evaluated to a single value.\n",
       "* It is a mathematical or logical statement that represents a numerical value or a truth value.\n",
       "* An expression can be a single value, a complex expression, or a compound expression.\n",
       "\n",
       "**Operator:**\n",
       "\n",
       "* An operator is a symbol that performs a specific operation on two or more operands.\n",
       "* It is used to combine values or to perform calculations.\n",
       "* An operator can be a binary operator (e.g"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "question = \"What is the difference between an expression and an operator?\"\n",
    "\n",
    "prompt = template.format(\n",
    "    instruction=question,\n",
    "    response=\"\",\n",
    ")\n",
    "\n",
    "response_text = generate_response(model, tokenizer, prompt, device, 128)\n",
    "\n",
    "Markdown(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Applying Gemma LoRA\n",
    "\n",
    "In this Session, we'll be applying the LoRA (**Low-Rank Adaptation**) technique to the **Gemma model**, a method designed to make fine-tuning large models like Gemma both **fast and efficient**. LoRA, a part of **PEFT** (**Parameter Efficient Fine-Tuning**), focuses on updating specific parts of a pre-trained model by only training a select few dense layers. This drastically cuts down on the computational demands and GPU memory needs, all without adding any extra time to the inference process. Here's what makes LoRA so powerful for our purposes:\n",
    "\n",
    "<center><img src=\"https://cdn-lfs.huggingface.co/datasets/huggingface/documentation-images/4313422c5f2755897fb8ddfc5b99251358f679647ec0f2d120a3f1ff060defe7?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27lora_diagram.png%3B+filename%3D%22lora_diagram.png%22%3B&response-content-type=image%2Fpng&Expires=1713275384&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMzI3NTM4NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9odWdnaW5nZmFjZS9kb2N1bWVudGF0aW9uLWltYWdlcy80MzEzNDIyYzVmMjc1NTg5N2ZiOGRkZmM1Yjk5MjUxMzU4ZjY3OTY0N2VjMGYyZDEyMGEzZjFmZjA2MGRlZmU3P3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=NAlgCQRn6ktvkOq8WpJkP7DyBvC3ta3Z5gGREWKvLDGQLYpypCszzucGL7nFdzirC4Py9CkgAgkAwbtGAkBU0JvbDVqxIAK9SzpX34xyFmoERdHqH2sQUh17cZ42f60MU9E%7E209I%7Ec6HgUNponN8lhoQzn0jEKYvkzsVsVUPu4OuYONDx4C1tywJIDovcKZCqEQY7f9-OjEKjLPr-CkNymcE%7Eprd83SMPThprA3HVl4gmMbCslQgUM8mM5imHcFxozdbzgD1Mb0U%7El7THXSeBWXdpGdZIBjbJSwJBEEMBtlVbbKtncPTrZWUjrrq03EJJSB7Cc8IA%7EgtJ3cbUerDGw__&Key-Pair-Id=KVTP0A1DKRTAX\" width=\"500\"><br/>\n",
    "Paper: <a href=\"https://arxiv.org/abs/2106.09685\">LoRA: Low-Rank Adaptation of Large Language Models</a></center>\n",
    "\n",
    "- **Dramatically reduces the number of parameters** needed, by up to **10,000 times**.\n",
    "- **Cuts down GPU memory usage** by **three times**.\n",
    "- **Maintains quick inference times** with **no additional latency**.\n",
    "\n",
    "The essence of PEFT, and by extension LoRA, is to enhance a model's performance using minimal resources, focusing on fine-tuning a handful of parameters for specific tasks. This technique is particularly advantageous as it:\n",
    "  \n",
    "- Optimizes rank decomposition matrices, maintaining the original model weights while adding optimized low-rank weights **A** and **B**.\n",
    "- Allows for up to **threefold reductions** in both time and computational costs.\n",
    "- Enables easy swapping of the LoRA module (weights **A** and **B**) according to the task at hand, lowering storage requirements and avoiding any increase in inference time.\n",
    "\n",
    "When applied specifically to **Transformer architectures**, targeting **attention weights** and keeping MLP modules static, LoRA significantly enhances the model's efficiency. For instance, in GPT-3 175B models, it:\n",
    "  \n",
    "- **Reduces VRAM usage** from **1.2TB to 350GB**.\n",
    "- **Lowers checkpoint size** from **350GB to 35MB**.\n",
    "- **Boosts training speed** by approximately **25%**.\n",
    "\n",
    "By integrating LoRA into Gemma, we aim to streamline the model's fine-tuning process in this Session, making it quicker and more resource-efficient, without compromising on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T17:19:02.046698Z",
     "iopub.status.busy": "2024-04-13T17:19:02.046069Z",
     "iopub.status.idle": "2024-04-13T17:19:02.051888Z",
     "shell.execute_reply": "2024-04-13T17:19:02.050874Z",
     "shell.execute_reply.started": "2024-04-13T17:19:02.046662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LoRA configuration: Sets up the parameters for Low-Rank Adaptation, which is a method for efficient fine-tuning of transformers.\n",
    "lora_config = LoraConfig(\n",
    "    r = 8,  # Rank of the adaptation matrices. A lower rank means fewer parameters to train.\n",
    "    target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],  # Transformer modules to apply LoRA.\n",
    "    task_type = \"CAUSAL_LM\",  # The type of task, here it is causal language modeling.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Training Gemma\n",
    "\n",
    "Now that everything is set up, it's time to finetune the Gemma model on your data. This section will guide you through the training process, including setting up your training loop and selecting the right hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T17:19:02.053215Z",
     "iopub.status.busy": "2024-04-13T17:19:02.052920Z",
     "iopub.status.idle": "2024-04-13T17:19:02.068689Z",
     "shell.execute_reply": "2024-04-13T17:19:02.067788Z",
     "shell.execute_reply.started": "2024-04-13T17:19:02.053190Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    \"\"\"\n",
    "    Formats a given example (a dictionary containing question and answer) using the predefined template.\n",
    "    \n",
    "    Parameters:\n",
    "    - example (dict): A dictionary with keys corresponding to the columns of the dataset, such as 'question' and 'answer'.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list containing a single formatted string that combines the instruction and the response.\n",
    "    \"\"\"\n",
    "    # Add the phrase to verify training success and format the text using the template and the specific example's instruction and response.\n",
    "    line = template.format(instruction=example[question_column], response=example[answer_column])\n",
    "    return [line]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T17:19:02.070337Z",
     "iopub.status.busy": "2024-04-13T17:19:02.069881Z",
     "iopub.status.idle": "2024-04-13T17:19:04.066929Z",
     "shell.execute_reply": "2024-04-13T17:19:04.065995Z",
     "shell.execute_reply.started": "2024-04-13T17:19:02.070304Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 17709/17709 [00:01<00:00, 17683.31 examples/s]\n",
      "d:\\Projects\\ByteMentor\\.venv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:294: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "  2%|▏         | 1/50 [00:07<05:54,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2666, 'grad_norm': 1.848942756652832, 'learning_rate': 0.05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:17<07:15,  9.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4792, 'grad_norm': 1.9833178520202637, 'learning_rate': 0.1, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:27<07:26,  9.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.8878, 'grad_norm': 477.66217041015625, 'learning_rate': 0.09791666666666667, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:37<07:28,  9.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 48.9532, 'grad_norm': 73.3408203125, 'learning_rate': 0.09583333333333334, 'epoch': 3.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:47<07:22,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 32.6048, 'grad_norm': 33.01348876953125, 'learning_rate': 0.09375, 'epoch': 4.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:57<07:17,  9.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1196.4397, 'grad_norm': 154.43783569335938, 'learning_rate': 0.09166666666666667, 'epoch': 5.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [01:07<07:06,  9.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 428.3225, 'grad_norm': 83.56133270263672, 'learning_rate': 0.08958333333333335, 'epoch': 6.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [01:17<06:57,  9.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 66.251, 'grad_norm': 69.5183334350586, 'learning_rate': 0.08750000000000001, 'epoch': 7.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [01:27<06:51, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 32.266, 'grad_norm': 7.729068279266357, 'learning_rate': 0.08541666666666667, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [01:37<06:41, 10.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 29.6726, 'grad_norm': 4.172179698944092, 'learning_rate': 0.08333333333333334, 'epoch': 8.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [01:48<06:33, 10.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 23.9157, 'grad_norm': 3.1237051486968994, 'learning_rate': 0.08125, 'epoch': 9.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [01:58<06:22, 10.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 18.8171, 'grad_norm': 3.3404932022094727, 'learning_rate': 0.07916666666666666, 'epoch': 10.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [02:08<06:14, 10.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 14.5371, 'grad_norm': 2.7405872344970703, 'learning_rate': 0.07708333333333334, 'epoch': 11.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [02:18<06:01, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.6202, 'grad_norm': 3.33337664604187, 'learning_rate': 0.07500000000000001, 'epoch': 12.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [02:28<05:52, 10.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 18.5069, 'grad_norm': 12.182422637939453, 'learning_rate': 0.07291666666666667, 'epoch': 13.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [02:38<05:40, 10.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 16.5787, 'grad_norm': 10.316153526306152, 'learning_rate': 0.07083333333333335, 'epoch': 14.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [02:48<05:35, 10.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.3676, 'grad_norm': 5.756263256072998, 'learning_rate': 0.06875, 'epoch': 15.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [02:59<05:30, 10.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.9383, 'grad_norm': 2.003077983856201, 'learning_rate': 0.06666666666666667, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [03:11<05:34, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.2738, 'grad_norm': 1.2141189575195312, 'learning_rate': 0.06458333333333334, 'epoch': 16.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [03:21<05:17, 10.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.1363, 'grad_norm': 1.485047459602356, 'learning_rate': 0.0625, 'epoch': 17.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [03:32<05:13, 10.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.4376, 'grad_norm': 1.0087316036224365, 'learning_rate': 0.06041666666666667, 'epoch': 18.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [03:43<05:04, 10.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.282, 'grad_norm': 2.505711555480957, 'learning_rate': 0.05833333333333334, 'epoch': 19.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [03:54<04:52, 10.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.1933, 'grad_norm': 0.8687736392021179, 'learning_rate': 0.05625, 'epoch': 20.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [04:05<04:38, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.6171, 'grad_norm': 0.6881597638130188, 'learning_rate': 0.05416666666666667, 'epoch': 21.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [04:16<04:30, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.0977, 'grad_norm': 0.658876895904541, 'learning_rate': 0.05208333333333334, 'epoch': 22.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [04:27<04:22, 10.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.2639, 'grad_norm': 0.7071253061294556, 'learning_rate': 0.05, 'epoch': 23.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [04:37<04:09, 10.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.3808, 'grad_norm': 0.7664394378662109, 'learning_rate': 0.04791666666666667, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [04:48<03:58, 10.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.7937, 'grad_norm': 0.5878800749778748, 'learning_rate': 0.04583333333333334, 'epoch': 24.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [04:59<03:47, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.8112, 'grad_norm': 0.5879973769187927, 'learning_rate': 0.043750000000000004, 'epoch': 25.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [05:10<03:36, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.6185, 'grad_norm': 0.8734429478645325, 'learning_rate': 0.04166666666666667, 'epoch': 26.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [05:21<03:26, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.4027, 'grad_norm': 0.5138237476348877, 'learning_rate': 0.03958333333333333, 'epoch': 27.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [05:32<03:15, 10.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.0045, 'grad_norm': 0.6346908211708069, 'learning_rate': 0.037500000000000006, 'epoch': 28.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [05:43<03:05, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.2034, 'grad_norm': 0.46172454953193665, 'learning_rate': 0.03541666666666667, 'epoch': 29.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [05:53<02:53, 10.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7808, 'grad_norm': 0.24861326813697815, 'learning_rate': 0.03333333333333333, 'epoch': 30.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [06:04<02:42, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7251, 'grad_norm': 0.7457124590873718, 'learning_rate': 0.03125, 'epoch': 31.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [06:15<02:31, 10.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6953, 'grad_norm': 0.5533333420753479, 'learning_rate': 0.02916666666666667, 'epoch': 32.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [06:26<02:20, 10.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5623, 'grad_norm': 0.25542348623275757, 'learning_rate': 0.027083333333333334, 'epoch': 32.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [06:37<02:10, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3959, 'grad_norm': 0.4699857532978058, 'learning_rate': 0.025, 'epoch': 33.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [06:48<01:59, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3666, 'grad_norm': 0.4870361089706421, 'learning_rate': 0.02291666666666667, 'epoch': 34.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [06:59<01:48, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3469, 'grad_norm': 0.3892308175563812, 'learning_rate': 0.020833333333333336, 'epoch': 35.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [07:09<01:37, 10.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1393, 'grad_norm': 0.24808067083358765, 'learning_rate': 0.018750000000000003, 'epoch': 36.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [07:20<01:27, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3527, 'grad_norm': 0.45500484108924866, 'learning_rate': 0.016666666666666666, 'epoch': 37.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [07:31<01:16, 10.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1583, 'grad_norm': 0.5202412009239197, 'learning_rate': 0.014583333333333335, 'epoch': 38.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [07:42<01:05, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.0541, 'grad_norm': 0.19677403569221497, 'learning_rate': 0.0125, 'epoch': 39.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [07:53<00:54, 10.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1647, 'grad_norm': 0.15965358912944794, 'learning_rate': 0.010416666666666668, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [08:04<00:43, 10.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1508, 'grad_norm': 0.2334556132555008, 'learning_rate': 0.008333333333333333, 'epoch': 40.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [08:15<00:32, 10.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.0817, 'grad_norm': 0.3202630877494812, 'learning_rate': 0.00625, 'epoch': 41.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [08:26<00:21, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.8803, 'grad_norm': 0.3207882046699524, 'learning_rate': 0.004166666666666667, 'epoch': 42.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [08:36<00:10, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3832, 'grad_norm': 0.33905747532844543, 'learning_rate': 0.0020833333333333333, 'epoch': 43.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [08:47<00:00, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.8375, 'grad_norm': 0.21071316301822662, 'learning_rate': 0.0, 'epoch': 44.44}\n",
      "{'train_runtime': 527.6552, 'train_samples_per_second': 1.516, 'train_steps_per_second': 0.095, 'train_loss': 44.400341215133665, 'epoch': 44.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=44.400341215133665, metrics={'train_runtime': 527.6552, 'train_samples_per_second': 1.516, 'train_steps_per_second': 0.095, 'train_loss': 44.400341215133665, 'epoch': 44.44})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    warmup_steps=2,\n",
    "    max_steps=50,\n",
    "    learning_rate=0.1,\n",
    "    logging_steps=1,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    output_dir=\"outputs\",\n",
    "    optim=\"adamw_torch_fused\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    max_seq_length=128,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Q&A Results After Finetuning\n",
    "\n",
    "After training, let's see how much our Gemma model has improved. We'll rerun the question-answering test and compare the results to the pre-finetuning performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T17:19:46.371158Z",
     "iopub.status.busy": "2024-04-13T17:19:46.370828Z",
     "iopub.status.idle": "2024-04-13T17:19:55.489935Z",
     "shell.execute_reply": "2024-04-13T17:19:55.489009Z",
     "shell.execute_reply.started": "2024-04-13T17:19:46.371132Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       " is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is the difference between an expression and an operator?\"\n",
    "\n",
    "prompt = template.format(\n",
    "    instruction=question,\n",
    "    response=\"\",\n",
    ")\n",
    "\n",
    "response_text = generate_response(trainer.model, tokenizer, prompt, device, 128)\n",
    "\n",
    "Markdown(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Although** the performance of the Gemma model was already outstanding, it might appear that there is not a significant difference after training. However, the value of this notebook lies in providing a comprehensive learning method for beginners. This is of great importance, and through this notebook, Gemma can also learn about topics it was previously unfamiliar with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Conclusion\n",
    "\n",
    "In this beginner-friendly notebook, we've outlined the process of fine-tuning the Gemma model, a Large Language Model (LLM), specifically for Python Q&A generation. Starting from data loading and preprocessing, we've demonstrated how to train the Gemma model effectively, even for those new to working with LLMs.\n",
    "\n",
    "We leveraged the Dataset_Python_Question_Answer, featuring hundreds of Python programming questions and answers, to train and refine the Gemma model's capabilities in generating accurate Q&As. This journey, while introductory, underscores the potential and straightforward path to engaging with LLMs through the Gemma model.\n",
    "\n",
    "Achieving the best performance with the Gemma model (or any LLM) generally requires training with more extensive datasets and over more epochs. Future enhancements could include integrating Retrieval-Augmented Generation (RAG) and Direct Preference Optimization (DPO) training techniques, offering a way to further improve the model by incorporating external knowledge bases for more precise and relevant responses.\n",
    "\n",
    "Ultimately, this notebook is designed to make the Gemma model approachable for beginners, illustrating that straightforward steps can unlock the potential of LLMs for diverse domain-specific tasks. It encourages users to experiment with the Gemma model across various fields, broadening the scope of its application and enhancing its utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>If you find this notebook useful, please consider upvoting it.</b> \n",
    "   \n",
    "<b>This will help others find it and encourage me to write more code, which benefits everyone.</b>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7669720,
     "sourceId": 64148,
     "sourceType": "competition"
    },
    {
     "datasetId": 4616621,
     "sourceId": 7970419,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 8318,
     "sourceId": 28785,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
