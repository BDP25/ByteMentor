{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4444.444444444444,
  "eval_steps": 500,
  "global_step": 5000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 8.89,
      "grad_norm": 65.98545837402344,
      "learning_rate": 0.01998399679935987,
      "loss": 441.4006,
      "step": 10
    },
    {
      "epoch": 17.78,
      "grad_norm": 33.9894905090332,
      "learning_rate": 0.01996399279855971,
      "loss": 27.1387,
      "step": 20
    },
    {
      "epoch": 26.67,
      "grad_norm": 3.0015273094177246,
      "learning_rate": 0.019943988797759552,
      "loss": 10.2108,
      "step": 30
    },
    {
      "epoch": 35.56,
      "grad_norm": 2.5647335052490234,
      "learning_rate": 0.01992398479695939,
      "loss": 6.6619,
      "step": 40
    },
    {
      "epoch": 44.44,
      "grad_norm": 1.60749089717865,
      "learning_rate": 0.01990398079615923,
      "loss": 5.8219,
      "step": 50
    },
    {
      "epoch": 53.33,
      "grad_norm": 1.2259613275527954,
      "learning_rate": 0.019883976795359075,
      "loss": 5.4947,
      "step": 60
    },
    {
      "epoch": 62.22,
      "grad_norm": 2.687499761581421,
      "learning_rate": 0.019863972794558912,
      "loss": 5.2792,
      "step": 70
    },
    {
      "epoch": 71.11,
      "grad_norm": 0.988903820514679,
      "learning_rate": 0.019843968793758753,
      "loss": 5.079,
      "step": 80
    },
    {
      "epoch": 80.0,
      "grad_norm": 1.8142716884613037,
      "learning_rate": 0.019823964792958594,
      "loss": 5.0353,
      "step": 90
    },
    {
      "epoch": 88.89,
      "grad_norm": 1.8486595153808594,
      "learning_rate": 0.01980396079215843,
      "loss": 5.1339,
      "step": 100
    },
    {
      "epoch": 97.78,
      "grad_norm": 1.6772412061691284,
      "learning_rate": 0.019783956791358272,
      "loss": 5.0514,
      "step": 110
    },
    {
      "epoch": 106.67,
      "grad_norm": 2.1314704418182373,
      "learning_rate": 0.019763952790558113,
      "loss": 5.0525,
      "step": 120
    },
    {
      "epoch": 115.56,
      "grad_norm": 1.2970131635665894,
      "learning_rate": 0.019743948789757954,
      "loss": 4.8485,
      "step": 130
    },
    {
      "epoch": 124.44,
      "grad_norm": 1.0381653308868408,
      "learning_rate": 0.01972394478895779,
      "loss": 4.8159,
      "step": 140
    },
    {
      "epoch": 133.33,
      "grad_norm": 1.6318837404251099,
      "learning_rate": 0.019703940788157633,
      "loss": 4.7396,
      "step": 150
    },
    {
      "epoch": 142.22,
      "grad_norm": 1.1313855648040771,
      "learning_rate": 0.019683936787357473,
      "loss": 4.6827,
      "step": 160
    },
    {
      "epoch": 151.11,
      "grad_norm": 1.739168405532837,
      "learning_rate": 0.01966393278655731,
      "loss": 4.7596,
      "step": 170
    },
    {
      "epoch": 160.0,
      "grad_norm": 1.1988967657089233,
      "learning_rate": 0.019643928785757152,
      "loss": 4.7444,
      "step": 180
    },
    {
      "epoch": 168.89,
      "grad_norm": 2.060953140258789,
      "learning_rate": 0.019623924784956993,
      "loss": 4.7586,
      "step": 190
    },
    {
      "epoch": 177.78,
      "grad_norm": 1.7686585187911987,
      "learning_rate": 0.019603920784156834,
      "loss": 4.9851,
      "step": 200
    },
    {
      "epoch": 186.67,
      "grad_norm": 0.8083450794219971,
      "learning_rate": 0.01958391678335667,
      "loss": 4.7831,
      "step": 210
    },
    {
      "epoch": 195.56,
      "grad_norm": 1.4370115995407104,
      "learning_rate": 0.019563912782556512,
      "loss": 4.7726,
      "step": 220
    },
    {
      "epoch": 204.44,
      "grad_norm": 0.7414931058883667,
      "learning_rate": 0.019543908781756353,
      "loss": 4.6722,
      "step": 230
    },
    {
      "epoch": 213.33,
      "grad_norm": 0.727716863155365,
      "learning_rate": 0.019523904780956194,
      "loss": 4.6639,
      "step": 240
    },
    {
      "epoch": 222.22,
      "grad_norm": 0.7294309735298157,
      "learning_rate": 0.01950390078015603,
      "loss": 4.627,
      "step": 250
    },
    {
      "epoch": 231.11,
      "grad_norm": 1.0143275260925293,
      "learning_rate": 0.019483896779355872,
      "loss": 4.601,
      "step": 260
    },
    {
      "epoch": 240.0,
      "grad_norm": 0.4093455970287323,
      "learning_rate": 0.019463892778555713,
      "loss": 4.6043,
      "step": 270
    },
    {
      "epoch": 248.89,
      "grad_norm": 0.751011312007904,
      "learning_rate": 0.01944388877775555,
      "loss": 4.5891,
      "step": 280
    },
    {
      "epoch": 257.78,
      "grad_norm": 1.1996963024139404,
      "learning_rate": 0.01942388477695539,
      "loss": 4.6164,
      "step": 290
    },
    {
      "epoch": 266.67,
      "grad_norm": 1.3264594078063965,
      "learning_rate": 0.019403880776155232,
      "loss": 4.6394,
      "step": 300
    },
    {
      "epoch": 275.56,
      "grad_norm": 1.1429764032363892,
      "learning_rate": 0.019383876775355073,
      "loss": 4.6584,
      "step": 310
    },
    {
      "epoch": 284.44,
      "grad_norm": 0.9230547547340393,
      "learning_rate": 0.01936387277455491,
      "loss": 4.6023,
      "step": 320
    },
    {
      "epoch": 293.33,
      "grad_norm": 1.7394442558288574,
      "learning_rate": 0.01934386877375475,
      "loss": 4.6615,
      "step": 330
    },
    {
      "epoch": 302.22,
      "grad_norm": 3.437539577484131,
      "learning_rate": 0.019323864772954592,
      "loss": 4.7019,
      "step": 340
    },
    {
      "epoch": 311.11,
      "grad_norm": 1.5984841585159302,
      "learning_rate": 0.01930386077215443,
      "loss": 4.657,
      "step": 350
    },
    {
      "epoch": 320.0,
      "grad_norm": 1.4337437152862549,
      "learning_rate": 0.01928385677135427,
      "loss": 4.6174,
      "step": 360
    },
    {
      "epoch": 328.89,
      "grad_norm": 1.0821812152862549,
      "learning_rate": 0.01926385277055411,
      "loss": 4.5921,
      "step": 370
    },
    {
      "epoch": 337.78,
      "grad_norm": 1.1679061651229858,
      "learning_rate": 0.019243848769753952,
      "loss": 4.5628,
      "step": 380
    },
    {
      "epoch": 346.67,
      "grad_norm": 0.4751288890838623,
      "learning_rate": 0.01922384476895379,
      "loss": 4.5567,
      "step": 390
    },
    {
      "epoch": 355.56,
      "grad_norm": 0.22486531734466553,
      "learning_rate": 0.01920384076815363,
      "loss": 4.5499,
      "step": 400
    },
    {
      "epoch": 364.44,
      "grad_norm": 0.5366932153701782,
      "learning_rate": 0.01918383676735347,
      "loss": 4.5434,
      "step": 410
    },
    {
      "epoch": 373.33,
      "grad_norm": 0.5594359040260315,
      "learning_rate": 0.01916383276655331,
      "loss": 4.5392,
      "step": 420
    },
    {
      "epoch": 382.22,
      "grad_norm": 0.7005259990692139,
      "learning_rate": 0.01914382876575315,
      "loss": 4.5411,
      "step": 430
    },
    {
      "epoch": 391.11,
      "grad_norm": 2.279384136199951,
      "learning_rate": 0.01912382476495299,
      "loss": 4.6413,
      "step": 440
    },
    {
      "epoch": 400.0,
      "grad_norm": 1.2858350276947021,
      "learning_rate": 0.019103820764152832,
      "loss": 4.6573,
      "step": 450
    },
    {
      "epoch": 408.89,
      "grad_norm": 0.799475908279419,
      "learning_rate": 0.01908381676335267,
      "loss": 4.5848,
      "step": 460
    },
    {
      "epoch": 417.78,
      "grad_norm": 0.70063316822052,
      "learning_rate": 0.01906381276255251,
      "loss": 4.5438,
      "step": 470
    },
    {
      "epoch": 426.67,
      "grad_norm": 1.1548160314559937,
      "learning_rate": 0.01904380876175235,
      "loss": 4.5388,
      "step": 480
    },
    {
      "epoch": 435.56,
      "grad_norm": 0.6320074200630188,
      "learning_rate": 0.019023804760952192,
      "loss": 4.5592,
      "step": 490
    },
    {
      "epoch": 444.44,
      "grad_norm": 0.6049333810806274,
      "learning_rate": 0.01900380076015203,
      "loss": 4.5427,
      "step": 500
    },
    {
      "epoch": 453.33,
      "grad_norm": 0.688112735748291,
      "learning_rate": 0.01898379675935187,
      "loss": 4.5383,
      "step": 510
    },
    {
      "epoch": 462.22,
      "grad_norm": 0.44513025879859924,
      "learning_rate": 0.01896379275855171,
      "loss": 4.5324,
      "step": 520
    },
    {
      "epoch": 471.11,
      "grad_norm": 0.6179673075675964,
      "learning_rate": 0.01894378875775155,
      "loss": 4.5538,
      "step": 530
    },
    {
      "epoch": 480.0,
      "grad_norm": 1.3946877717971802,
      "learning_rate": 0.01892378475695139,
      "loss": 4.5552,
      "step": 540
    },
    {
      "epoch": 488.89,
      "grad_norm": 1.8343005180358887,
      "learning_rate": 0.01890378075615123,
      "loss": 4.6155,
      "step": 550
    },
    {
      "epoch": 497.78,
      "grad_norm": 2.7664878368377686,
      "learning_rate": 0.01888377675535107,
      "loss": 4.7264,
      "step": 560
    },
    {
      "epoch": 506.67,
      "grad_norm": 1.8192332983016968,
      "learning_rate": 0.01886377275455091,
      "loss": 4.688,
      "step": 570
    },
    {
      "epoch": 515.56,
      "grad_norm": 0.9530156850814819,
      "learning_rate": 0.01884376875375075,
      "loss": 4.6188,
      "step": 580
    },
    {
      "epoch": 524.44,
      "grad_norm": 0.39021036028862,
      "learning_rate": 0.01882376475295059,
      "loss": 4.5808,
      "step": 590
    },
    {
      "epoch": 533.33,
      "grad_norm": 1.167852759361267,
      "learning_rate": 0.018803760752150428,
      "loss": 4.5614,
      "step": 600
    },
    {
      "epoch": 542.22,
      "grad_norm": 0.4460256099700928,
      "learning_rate": 0.01878375675135027,
      "loss": 4.5313,
      "step": 610
    },
    {
      "epoch": 551.11,
      "grad_norm": 1.418631911277771,
      "learning_rate": 0.01876375275055011,
      "loss": 4.5478,
      "step": 620
    },
    {
      "epoch": 560.0,
      "grad_norm": 0.18161480128765106,
      "learning_rate": 0.01874374874974995,
      "loss": 4.522,
      "step": 630
    },
    {
      "epoch": 568.89,
      "grad_norm": 0.2439957559108734,
      "learning_rate": 0.01872374474894979,
      "loss": 4.5241,
      "step": 640
    },
    {
      "epoch": 577.78,
      "grad_norm": 0.4863653779029846,
      "learning_rate": 0.018703740748149632,
      "loss": 4.5389,
      "step": 650
    },
    {
      "epoch": 586.67,
      "grad_norm": 1.0733739137649536,
      "learning_rate": 0.01868373674734947,
      "loss": 4.5314,
      "step": 660
    },
    {
      "epoch": 595.56,
      "grad_norm": 0.9615392088890076,
      "learning_rate": 0.01866373274654931,
      "loss": 4.5848,
      "step": 670
    },
    {
      "epoch": 604.44,
      "grad_norm": 0.2585769295692444,
      "learning_rate": 0.01864372874574915,
      "loss": 4.539,
      "step": 680
    },
    {
      "epoch": 613.33,
      "grad_norm": 0.2482437640428543,
      "learning_rate": 0.018623724744948993,
      "loss": 4.5272,
      "step": 690
    },
    {
      "epoch": 622.22,
      "grad_norm": 0.9530606269836426,
      "learning_rate": 0.01860372074414883,
      "loss": 4.5366,
      "step": 700
    },
    {
      "epoch": 631.11,
      "grad_norm": 0.6103882193565369,
      "learning_rate": 0.01858371674334867,
      "loss": 4.5373,
      "step": 710
    },
    {
      "epoch": 640.0,
      "grad_norm": 0.20059163868427277,
      "learning_rate": 0.018563712742548512,
      "loss": 4.5269,
      "step": 720
    },
    {
      "epoch": 648.89,
      "grad_norm": 0.46241751313209534,
      "learning_rate": 0.01854370874174835,
      "loss": 4.5116,
      "step": 730
    },
    {
      "epoch": 657.78,
      "grad_norm": 0.1321820616722107,
      "learning_rate": 0.01852370474094819,
      "loss": 4.5258,
      "step": 740
    },
    {
      "epoch": 666.67,
      "grad_norm": 0.2174443006515503,
      "learning_rate": 0.01850370074014803,
      "loss": 4.5149,
      "step": 750
    },
    {
      "epoch": 675.56,
      "grad_norm": 0.22228355705738068,
      "learning_rate": 0.018483696739347872,
      "loss": 4.5279,
      "step": 760
    },
    {
      "epoch": 684.44,
      "grad_norm": 0.5005280375480652,
      "learning_rate": 0.01846369273854771,
      "loss": 4.5221,
      "step": 770
    },
    {
      "epoch": 693.33,
      "grad_norm": 0.27495115995407104,
      "learning_rate": 0.01844368873774755,
      "loss": 4.5232,
      "step": 780
    },
    {
      "epoch": 702.22,
      "grad_norm": 1.169701337814331,
      "learning_rate": 0.01842368473694739,
      "loss": 4.5678,
      "step": 790
    },
    {
      "epoch": 711.11,
      "grad_norm": 0.27680909633636475,
      "learning_rate": 0.01840368073614723,
      "loss": 4.5595,
      "step": 800
    },
    {
      "epoch": 720.0,
      "grad_norm": 0.40345367789268494,
      "learning_rate": 0.01838367673534707,
      "loss": 4.5255,
      "step": 810
    },
    {
      "epoch": 728.89,
      "grad_norm": 0.311181902885437,
      "learning_rate": 0.01836367273454691,
      "loss": 4.5291,
      "step": 820
    },
    {
      "epoch": 737.78,
      "grad_norm": 2.0184993743896484,
      "learning_rate": 0.01834366873374675,
      "loss": 4.584,
      "step": 830
    },
    {
      "epoch": 746.67,
      "grad_norm": 1.0627332925796509,
      "learning_rate": 0.01832366473294659,
      "loss": 4.5339,
      "step": 840
    },
    {
      "epoch": 755.56,
      "grad_norm": 0.2796752154827118,
      "learning_rate": 0.01830366073214643,
      "loss": 4.5366,
      "step": 850
    },
    {
      "epoch": 764.44,
      "grad_norm": 1.1842446327209473,
      "learning_rate": 0.01828365673134627,
      "loss": 4.5266,
      "step": 860
    },
    {
      "epoch": 773.33,
      "grad_norm": 1.7807694673538208,
      "learning_rate": 0.01826365273054611,
      "loss": 4.5994,
      "step": 870
    },
    {
      "epoch": 782.22,
      "grad_norm": 0.7993282079696655,
      "learning_rate": 0.01824364872974595,
      "loss": 4.6631,
      "step": 880
    },
    {
      "epoch": 791.11,
      "grad_norm": 0.8198614716529846,
      "learning_rate": 0.01822364472894579,
      "loss": 4.5812,
      "step": 890
    },
    {
      "epoch": 800.0,
      "grad_norm": 0.29625046253204346,
      "learning_rate": 0.01820364072814563,
      "loss": 4.5375,
      "step": 900
    },
    {
      "epoch": 808.89,
      "grad_norm": 0.2961278557777405,
      "learning_rate": 0.018183636727345468,
      "loss": 4.5312,
      "step": 910
    },
    {
      "epoch": 817.78,
      "grad_norm": 1.0063852071762085,
      "learning_rate": 0.01816363272654531,
      "loss": 4.5358,
      "step": 920
    },
    {
      "epoch": 826.67,
      "grad_norm": 0.9080193638801575,
      "learning_rate": 0.01814362872574515,
      "loss": 4.5287,
      "step": 930
    },
    {
      "epoch": 835.56,
      "grad_norm": 0.49396851658821106,
      "learning_rate": 0.01812362472494499,
      "loss": 4.5226,
      "step": 940
    },
    {
      "epoch": 844.44,
      "grad_norm": 0.9020172953605652,
      "learning_rate": 0.018103620724144828,
      "loss": 4.5201,
      "step": 950
    },
    {
      "epoch": 853.33,
      "grad_norm": 2.337703227996826,
      "learning_rate": 0.01808361672334467,
      "loss": 4.5522,
      "step": 960
    },
    {
      "epoch": 862.22,
      "grad_norm": 0.3639414608478546,
      "learning_rate": 0.01806361272254451,
      "loss": 4.5232,
      "step": 970
    },
    {
      "epoch": 871.11,
      "grad_norm": 0.7137291431427002,
      "learning_rate": 0.018043608721744347,
      "loss": 4.5198,
      "step": 980
    },
    {
      "epoch": 880.0,
      "grad_norm": 0.33538979291915894,
      "learning_rate": 0.01802360472094419,
      "loss": 4.5289,
      "step": 990
    },
    {
      "epoch": 888.89,
      "grad_norm": 0.20478245615959167,
      "learning_rate": 0.01800360072014403,
      "loss": 4.524,
      "step": 1000
    },
    {
      "epoch": 897.78,
      "grad_norm": 0.5724456906318665,
      "learning_rate": 0.01798359671934387,
      "loss": 4.5192,
      "step": 1010
    },
    {
      "epoch": 906.67,
      "grad_norm": 0.5214424729347229,
      "learning_rate": 0.017963592718543708,
      "loss": 4.5264,
      "step": 1020
    },
    {
      "epoch": 915.56,
      "grad_norm": 0.27349427342414856,
      "learning_rate": 0.01794358871774355,
      "loss": 4.519,
      "step": 1030
    },
    {
      "epoch": 924.44,
      "grad_norm": 0.15590474009513855,
      "learning_rate": 0.01792358471694339,
      "loss": 4.519,
      "step": 1040
    },
    {
      "epoch": 933.33,
      "grad_norm": 0.2163034975528717,
      "learning_rate": 0.017903580716143227,
      "loss": 4.5191,
      "step": 1050
    },
    {
      "epoch": 942.22,
      "grad_norm": 0.24535566568374634,
      "learning_rate": 0.017883576715343068,
      "loss": 4.5194,
      "step": 1060
    },
    {
      "epoch": 951.11,
      "grad_norm": 0.47430527210235596,
      "learning_rate": 0.01786357271454291,
      "loss": 4.523,
      "step": 1070
    },
    {
      "epoch": 960.0,
      "grad_norm": 0.5284223556518555,
      "learning_rate": 0.01784356871374275,
      "loss": 4.5286,
      "step": 1080
    },
    {
      "epoch": 968.89,
      "grad_norm": 0.27017590403556824,
      "learning_rate": 0.017823564712942587,
      "loss": 4.5117,
      "step": 1090
    },
    {
      "epoch": 977.78,
      "grad_norm": 0.15518087148666382,
      "learning_rate": 0.017803560712142428,
      "loss": 4.5201,
      "step": 1100
    },
    {
      "epoch": 986.67,
      "grad_norm": 1.0195223093032837,
      "learning_rate": 0.01778355671134227,
      "loss": 4.5221,
      "step": 1110
    },
    {
      "epoch": 995.56,
      "grad_norm": 1.9419569969177246,
      "learning_rate": 0.01776355271054211,
      "loss": 4.5609,
      "step": 1120
    },
    {
      "epoch": 1004.44,
      "grad_norm": 1.3542367219924927,
      "learning_rate": 0.017743548709741947,
      "loss": 4.5662,
      "step": 1130
    },
    {
      "epoch": 1013.33,
      "grad_norm": 1.468176245689392,
      "learning_rate": 0.017723544708941788,
      "loss": 4.6019,
      "step": 1140
    },
    {
      "epoch": 1022.22,
      "grad_norm": 0.2655835747718811,
      "learning_rate": 0.01770354070814163,
      "loss": 4.5533,
      "step": 1150
    },
    {
      "epoch": 1031.11,
      "grad_norm": 0.3383360803127289,
      "learning_rate": 0.017683536707341466,
      "loss": 4.5332,
      "step": 1160
    },
    {
      "epoch": 1040.0,
      "grad_norm": 0.31943610310554504,
      "learning_rate": 0.017663532706541307,
      "loss": 4.51,
      "step": 1170
    },
    {
      "epoch": 1048.89,
      "grad_norm": 0.4272260367870331,
      "learning_rate": 0.017643528705741148,
      "loss": 4.5169,
      "step": 1180
    },
    {
      "epoch": 1057.78,
      "grad_norm": 0.40432220697402954,
      "learning_rate": 0.01762352470494099,
      "loss": 4.5152,
      "step": 1190
    },
    {
      "epoch": 1066.67,
      "grad_norm": 0.49215269088745117,
      "learning_rate": 0.01760352070414083,
      "loss": 4.5327,
      "step": 1200
    },
    {
      "epoch": 1075.56,
      "grad_norm": 0.2930447459220886,
      "learning_rate": 0.01758351670334067,
      "loss": 4.5332,
      "step": 1210
    },
    {
      "epoch": 1084.44,
      "grad_norm": 0.4606751501560211,
      "learning_rate": 0.01756351270254051,
      "loss": 4.53,
      "step": 1220
    },
    {
      "epoch": 1093.33,
      "grad_norm": 0.2900943458080292,
      "learning_rate": 0.01754350870174035,
      "loss": 4.5235,
      "step": 1230
    },
    {
      "epoch": 1102.22,
      "grad_norm": 0.2342415302991867,
      "learning_rate": 0.01752350470094019,
      "loss": 4.5388,
      "step": 1240
    },
    {
      "epoch": 1111.11,
      "grad_norm": 1.8567830324172974,
      "learning_rate": 0.01750350070014003,
      "loss": 4.536,
      "step": 1250
    },
    {
      "epoch": 1120.0,
      "grad_norm": 0.7368165254592896,
      "learning_rate": 0.01748349669933987,
      "loss": 4.5401,
      "step": 1260
    },
    {
      "epoch": 1128.89,
      "grad_norm": 0.3745003044605255,
      "learning_rate": 0.01746349269853971,
      "loss": 4.5236,
      "step": 1270
    },
    {
      "epoch": 1137.78,
      "grad_norm": 0.13683201372623444,
      "learning_rate": 0.01744348869773955,
      "loss": 4.519,
      "step": 1280
    },
    {
      "epoch": 1146.67,
      "grad_norm": 0.23360934853553772,
      "learning_rate": 0.017423484696939388,
      "loss": 4.5273,
      "step": 1290
    },
    {
      "epoch": 1155.56,
      "grad_norm": 0.2472328543663025,
      "learning_rate": 0.01740348069613923,
      "loss": 4.5182,
      "step": 1300
    },
    {
      "epoch": 1164.44,
      "grad_norm": 0.6990105509757996,
      "learning_rate": 0.01738347669533907,
      "loss": 4.5205,
      "step": 1310
    },
    {
      "epoch": 1173.33,
      "grad_norm": 0.797112226486206,
      "learning_rate": 0.01736347269453891,
      "loss": 4.5553,
      "step": 1320
    },
    {
      "epoch": 1182.22,
      "grad_norm": 1.065966010093689,
      "learning_rate": 0.017343468693738748,
      "loss": 4.5169,
      "step": 1330
    },
    {
      "epoch": 1191.11,
      "grad_norm": 0.2676912248134613,
      "learning_rate": 0.01732346469293859,
      "loss": 4.5254,
      "step": 1340
    },
    {
      "epoch": 1200.0,
      "grad_norm": 0.44713330268859863,
      "learning_rate": 0.01730346069213843,
      "loss": 4.5269,
      "step": 1350
    },
    {
      "epoch": 1208.89,
      "grad_norm": 0.4030798673629761,
      "learning_rate": 0.017283456691338267,
      "loss": 4.5232,
      "step": 1360
    },
    {
      "epoch": 1217.78,
      "grad_norm": 0.16318076848983765,
      "learning_rate": 0.017263452690538108,
      "loss": 4.5104,
      "step": 1370
    },
    {
      "epoch": 1226.67,
      "grad_norm": 0.22375911474227905,
      "learning_rate": 0.01724344868973795,
      "loss": 4.516,
      "step": 1380
    },
    {
      "epoch": 1235.56,
      "grad_norm": 0.17881302535533905,
      "learning_rate": 0.01722344468893779,
      "loss": 4.5137,
      "step": 1390
    },
    {
      "epoch": 1244.44,
      "grad_norm": 0.7119426727294922,
      "learning_rate": 0.017203440688137627,
      "loss": 4.5048,
      "step": 1400
    },
    {
      "epoch": 1253.33,
      "grad_norm": 0.4524706304073334,
      "learning_rate": 0.017183436687337468,
      "loss": 4.5373,
      "step": 1410
    },
    {
      "epoch": 1262.22,
      "grad_norm": 0.17671968042850494,
      "learning_rate": 0.01716343268653731,
      "loss": 4.5358,
      "step": 1420
    },
    {
      "epoch": 1271.11,
      "grad_norm": 0.8386442065238953,
      "learning_rate": 0.017143428685737146,
      "loss": 4.5247,
      "step": 1430
    },
    {
      "epoch": 1280.0,
      "grad_norm": 0.32562491297721863,
      "learning_rate": 0.017123424684936987,
      "loss": 4.5154,
      "step": 1440
    },
    {
      "epoch": 1288.89,
      "grad_norm": 0.25800687074661255,
      "learning_rate": 0.017103420684136828,
      "loss": 4.5176,
      "step": 1450
    },
    {
      "epoch": 1297.78,
      "grad_norm": 0.5592182278633118,
      "learning_rate": 0.01708341668333667,
      "loss": 4.5152,
      "step": 1460
    },
    {
      "epoch": 1306.67,
      "grad_norm": 0.23738908767700195,
      "learning_rate": 0.017063412682536507,
      "loss": 4.5272,
      "step": 1470
    },
    {
      "epoch": 1315.56,
      "grad_norm": 0.46788638830184937,
      "learning_rate": 0.017043408681736347,
      "loss": 4.5194,
      "step": 1480
    },
    {
      "epoch": 1324.44,
      "grad_norm": 0.45880067348480225,
      "learning_rate": 0.01702340468093619,
      "loss": 4.5108,
      "step": 1490
    },
    {
      "epoch": 1333.33,
      "grad_norm": 0.24531476199626923,
      "learning_rate": 0.01700340068013603,
      "loss": 4.514,
      "step": 1500
    },
    {
      "epoch": 1342.22,
      "grad_norm": 0.3526315689086914,
      "learning_rate": 0.016983396679335867,
      "loss": 4.5108,
      "step": 1510
    },
    {
      "epoch": 1351.11,
      "grad_norm": 0.11795738339424133,
      "learning_rate": 0.016963392678535708,
      "loss": 4.5164,
      "step": 1520
    },
    {
      "epoch": 1360.0,
      "grad_norm": 0.23315370082855225,
      "learning_rate": 0.01694338867773555,
      "loss": 4.5125,
      "step": 1530
    },
    {
      "epoch": 1368.89,
      "grad_norm": 0.3125748336315155,
      "learning_rate": 0.016923384676935386,
      "loss": 4.5148,
      "step": 1540
    },
    {
      "epoch": 1377.78,
      "grad_norm": 0.19457414746284485,
      "learning_rate": 0.016903380676135227,
      "loss": 4.5216,
      "step": 1550
    },
    {
      "epoch": 1386.67,
      "grad_norm": 0.3264983594417572,
      "learning_rate": 0.016883376675335068,
      "loss": 4.5224,
      "step": 1560
    },
    {
      "epoch": 1395.56,
      "grad_norm": 0.3688790500164032,
      "learning_rate": 0.01686337267453491,
      "loss": 4.5111,
      "step": 1570
    },
    {
      "epoch": 1404.44,
      "grad_norm": 0.21397563815116882,
      "learning_rate": 0.016843368673734746,
      "loss": 4.5215,
      "step": 1580
    },
    {
      "epoch": 1413.33,
      "grad_norm": 0.24291811883449554,
      "learning_rate": 0.016823364672934587,
      "loss": 4.5217,
      "step": 1590
    },
    {
      "epoch": 1422.22,
      "grad_norm": 0.2998032569885254,
      "learning_rate": 0.016803360672134428,
      "loss": 4.5251,
      "step": 1600
    },
    {
      "epoch": 1431.11,
      "grad_norm": 0.2733798623085022,
      "learning_rate": 0.016783356671334265,
      "loss": 4.5161,
      "step": 1610
    },
    {
      "epoch": 1440.0,
      "grad_norm": 0.4095969498157501,
      "learning_rate": 0.016763352670534106,
      "loss": 4.5205,
      "step": 1620
    },
    {
      "epoch": 1448.89,
      "grad_norm": 0.10608330368995667,
      "learning_rate": 0.016743348669733947,
      "loss": 4.5182,
      "step": 1630
    },
    {
      "epoch": 1457.78,
      "grad_norm": 0.21910277009010315,
      "learning_rate": 0.016723344668933788,
      "loss": 4.5083,
      "step": 1640
    },
    {
      "epoch": 1466.67,
      "grad_norm": 0.5095503330230713,
      "learning_rate": 0.016703340668133625,
      "loss": 4.5187,
      "step": 1650
    },
    {
      "epoch": 1475.56,
      "grad_norm": 0.3758549392223358,
      "learning_rate": 0.016683336667333466,
      "loss": 4.5162,
      "step": 1660
    },
    {
      "epoch": 1484.44,
      "grad_norm": 0.5377249717712402,
      "learning_rate": 0.016663332666533307,
      "loss": 4.5153,
      "step": 1670
    },
    {
      "epoch": 1493.33,
      "grad_norm": 0.34621962904930115,
      "learning_rate": 0.016643328665733145,
      "loss": 4.6503,
      "step": 1680
    },
    {
      "epoch": 1502.22,
      "grad_norm": 1.3236812353134155,
      "learning_rate": 0.016623324664932985,
      "loss": 4.5744,
      "step": 1690
    },
    {
      "epoch": 1511.11,
      "grad_norm": 0.558176577091217,
      "learning_rate": 0.016603320664132826,
      "loss": 4.5459,
      "step": 1700
    },
    {
      "epoch": 1520.0,
      "grad_norm": 0.9203667640686035,
      "learning_rate": 0.016583316663332667,
      "loss": 4.5472,
      "step": 1710
    },
    {
      "epoch": 1528.89,
      "grad_norm": 0.9420191645622253,
      "learning_rate": 0.016563312662532505,
      "loss": 4.5405,
      "step": 1720
    },
    {
      "epoch": 1537.78,
      "grad_norm": 0.49586692452430725,
      "learning_rate": 0.016543308661732346,
      "loss": 4.5394,
      "step": 1730
    },
    {
      "epoch": 1546.67,
      "grad_norm": NaN,
      "learning_rate": 0.016523304660932187,
      "loss": 9.2026,
      "step": 1740
    },
    {
      "epoch": 1555.56,
      "grad_norm": NaN,
      "learning_rate": 0.016503300660132024,
      "loss": 0.0,
      "step": 1750
    },
    {
      "epoch": 1564.44,
      "grad_norm": NaN,
      "learning_rate": 0.016483296659331865,
      "loss": 0.0,
      "step": 1760
    },
    {
      "epoch": 1573.33,
      "grad_norm": NaN,
      "learning_rate": 0.01646329265853171,
      "loss": 0.0,
      "step": 1770
    },
    {
      "epoch": 1582.22,
      "grad_norm": NaN,
      "learning_rate": 0.016443288657731547,
      "loss": 0.0,
      "step": 1780
    },
    {
      "epoch": 1591.11,
      "grad_norm": NaN,
      "learning_rate": 0.016423284656931388,
      "loss": 0.0,
      "step": 1790
    },
    {
      "epoch": 1600.0,
      "grad_norm": NaN,
      "learning_rate": 0.01640328065613123,
      "loss": 0.0,
      "step": 1800
    },
    {
      "epoch": 1608.89,
      "grad_norm": NaN,
      "learning_rate": 0.016383276655331066,
      "loss": 0.0,
      "step": 1810
    },
    {
      "epoch": 1617.78,
      "grad_norm": NaN,
      "learning_rate": 0.016363272654530907,
      "loss": 0.0,
      "step": 1820
    },
    {
      "epoch": 1626.67,
      "grad_norm": NaN,
      "learning_rate": 0.016343268653730748,
      "loss": 0.0,
      "step": 1830
    },
    {
      "epoch": 1635.56,
      "grad_norm": NaN,
      "learning_rate": 0.01632326465293059,
      "loss": 0.0,
      "step": 1840
    },
    {
      "epoch": 1644.44,
      "grad_norm": NaN,
      "learning_rate": 0.016303260652130426,
      "loss": 0.0,
      "step": 1850
    },
    {
      "epoch": 1653.33,
      "grad_norm": NaN,
      "learning_rate": 0.016283256651330267,
      "loss": 0.0,
      "step": 1860
    },
    {
      "epoch": 1662.22,
      "grad_norm": NaN,
      "learning_rate": 0.016263252650530108,
      "loss": 0.0,
      "step": 1870
    },
    {
      "epoch": 1671.11,
      "grad_norm": NaN,
      "learning_rate": 0.016243248649729945,
      "loss": 0.0,
      "step": 1880
    },
    {
      "epoch": 1680.0,
      "grad_norm": NaN,
      "learning_rate": 0.016223244648929786,
      "loss": 0.0,
      "step": 1890
    },
    {
      "epoch": 1688.89,
      "grad_norm": NaN,
      "learning_rate": 0.016203240648129627,
      "loss": 0.0,
      "step": 1900
    },
    {
      "epoch": 1697.78,
      "grad_norm": NaN,
      "learning_rate": 0.016183236647329468,
      "loss": 0.0,
      "step": 1910
    },
    {
      "epoch": 1706.67,
      "grad_norm": NaN,
      "learning_rate": 0.016163232646529305,
      "loss": 0.0,
      "step": 1920
    },
    {
      "epoch": 1715.56,
      "grad_norm": NaN,
      "learning_rate": 0.016143228645729146,
      "loss": 0.0,
      "step": 1930
    },
    {
      "epoch": 1724.44,
      "grad_norm": NaN,
      "learning_rate": 0.016123224644928987,
      "loss": 0.0,
      "step": 1940
    },
    {
      "epoch": 1733.33,
      "grad_norm": NaN,
      "learning_rate": 0.016103220644128828,
      "loss": 0.0,
      "step": 1950
    },
    {
      "epoch": 1742.22,
      "grad_norm": NaN,
      "learning_rate": 0.016083216643328666,
      "loss": 0.0,
      "step": 1960
    },
    {
      "epoch": 1751.11,
      "grad_norm": NaN,
      "learning_rate": 0.016063212642528506,
      "loss": 0.0,
      "step": 1970
    },
    {
      "epoch": 1760.0,
      "grad_norm": NaN,
      "learning_rate": 0.016043208641728347,
      "loss": 0.0,
      "step": 1980
    },
    {
      "epoch": 1768.89,
      "grad_norm": NaN,
      "learning_rate": 0.016023204640928185,
      "loss": 0.0,
      "step": 1990
    },
    {
      "epoch": 1777.78,
      "grad_norm": NaN,
      "learning_rate": 0.016003200640128026,
      "loss": 0.0,
      "step": 2000
    },
    {
      "epoch": 1786.67,
      "grad_norm": NaN,
      "learning_rate": 0.015983196639327867,
      "loss": 0.0,
      "step": 2010
    },
    {
      "epoch": 1795.56,
      "grad_norm": NaN,
      "learning_rate": 0.015963192638527707,
      "loss": 0.0,
      "step": 2020
    },
    {
      "epoch": 1804.44,
      "grad_norm": NaN,
      "learning_rate": 0.015943188637727545,
      "loss": 0.0,
      "step": 2030
    },
    {
      "epoch": 1813.33,
      "grad_norm": NaN,
      "learning_rate": 0.015923184636927386,
      "loss": 0.0,
      "step": 2040
    },
    {
      "epoch": 1822.22,
      "grad_norm": NaN,
      "learning_rate": 0.015903180636127227,
      "loss": 0.0,
      "step": 2050
    },
    {
      "epoch": 1831.11,
      "grad_norm": NaN,
      "learning_rate": 0.015883176635327064,
      "loss": 0.0,
      "step": 2060
    },
    {
      "epoch": 1840.0,
      "grad_norm": NaN,
      "learning_rate": 0.015863172634526905,
      "loss": 0.0,
      "step": 2070
    },
    {
      "epoch": 1848.89,
      "grad_norm": NaN,
      "learning_rate": 0.015843168633726746,
      "loss": 0.0,
      "step": 2080
    },
    {
      "epoch": 1857.78,
      "grad_norm": NaN,
      "learning_rate": 0.015823164632926587,
      "loss": 0.0,
      "step": 2090
    },
    {
      "epoch": 1866.67,
      "grad_norm": NaN,
      "learning_rate": 0.015803160632126424,
      "loss": 0.0,
      "step": 2100
    },
    {
      "epoch": 1875.56,
      "grad_norm": NaN,
      "learning_rate": 0.015783156631326265,
      "loss": 0.0,
      "step": 2110
    },
    {
      "epoch": 1884.44,
      "grad_norm": NaN,
      "learning_rate": 0.015763152630526106,
      "loss": 0.0,
      "step": 2120
    },
    {
      "epoch": 1893.33,
      "grad_norm": NaN,
      "learning_rate": 0.015743148629725943,
      "loss": 0.0,
      "step": 2130
    },
    {
      "epoch": 1902.22,
      "grad_norm": NaN,
      "learning_rate": 0.015723144628925784,
      "loss": 0.0,
      "step": 2140
    },
    {
      "epoch": 1911.11,
      "grad_norm": NaN,
      "learning_rate": 0.015703140628125625,
      "loss": 0.0,
      "step": 2150
    },
    {
      "epoch": 1920.0,
      "grad_norm": NaN,
      "learning_rate": 0.015683136627325466,
      "loss": 0.0,
      "step": 2160
    },
    {
      "epoch": 1928.89,
      "grad_norm": NaN,
      "learning_rate": 0.015663132626525304,
      "loss": 0.0,
      "step": 2170
    },
    {
      "epoch": 1937.78,
      "grad_norm": NaN,
      "learning_rate": 0.015643128625725145,
      "loss": 0.0,
      "step": 2180
    },
    {
      "epoch": 1946.67,
      "grad_norm": NaN,
      "learning_rate": 0.015623124624924985,
      "loss": 0.0,
      "step": 2190
    },
    {
      "epoch": 1955.56,
      "grad_norm": NaN,
      "learning_rate": 0.015603120624124825,
      "loss": 0.0,
      "step": 2200
    },
    {
      "epoch": 1964.44,
      "grad_norm": NaN,
      "learning_rate": 0.015583116623324665,
      "loss": 0.0,
      "step": 2210
    },
    {
      "epoch": 1973.33,
      "grad_norm": NaN,
      "learning_rate": 0.015563112622524505,
      "loss": 0.0,
      "step": 2220
    },
    {
      "epoch": 1982.22,
      "grad_norm": NaN,
      "learning_rate": 0.015543108621724344,
      "loss": 0.0,
      "step": 2230
    },
    {
      "epoch": 1991.11,
      "grad_norm": NaN,
      "learning_rate": 0.015523104620924185,
      "loss": 0.0,
      "step": 2240
    },
    {
      "epoch": 2000.0,
      "grad_norm": NaN,
      "learning_rate": 0.015503100620124024,
      "loss": 0.0,
      "step": 2250
    },
    {
      "epoch": 2008.89,
      "grad_norm": NaN,
      "learning_rate": 0.015483096619323865,
      "loss": 0.0,
      "step": 2260
    },
    {
      "epoch": 2017.78,
      "grad_norm": NaN,
      "learning_rate": 0.015463092618523704,
      "loss": 0.0,
      "step": 2270
    },
    {
      "epoch": 2026.67,
      "grad_norm": NaN,
      "learning_rate": 0.015443088617723545,
      "loss": 0.0,
      "step": 2280
    },
    {
      "epoch": 2035.56,
      "grad_norm": NaN,
      "learning_rate": 0.015423084616923384,
      "loss": 0.0,
      "step": 2290
    },
    {
      "epoch": 2044.44,
      "grad_norm": NaN,
      "learning_rate": 0.015403080616123223,
      "loss": 0.0,
      "step": 2300
    },
    {
      "epoch": 2053.33,
      "grad_norm": NaN,
      "learning_rate": 0.015383076615323064,
      "loss": 0.0,
      "step": 2310
    },
    {
      "epoch": 2062.22,
      "grad_norm": NaN,
      "learning_rate": 0.015363072614522903,
      "loss": 0.0,
      "step": 2320
    },
    {
      "epoch": 2071.11,
      "grad_norm": NaN,
      "learning_rate": 0.015343068613722744,
      "loss": 0.0,
      "step": 2330
    },
    {
      "epoch": 2080.0,
      "grad_norm": NaN,
      "learning_rate": 0.015323064612922587,
      "loss": 0.0,
      "step": 2340
    },
    {
      "epoch": 2088.89,
      "grad_norm": NaN,
      "learning_rate": 0.015303060612122426,
      "loss": 0.0,
      "step": 2350
    },
    {
      "epoch": 2097.78,
      "grad_norm": NaN,
      "learning_rate": 0.015283056611322265,
      "loss": 0.0,
      "step": 2360
    },
    {
      "epoch": 2106.67,
      "grad_norm": NaN,
      "learning_rate": 0.015263052610522106,
      "loss": 0.0,
      "step": 2370
    },
    {
      "epoch": 2115.56,
      "grad_norm": NaN,
      "learning_rate": 0.015243048609721945,
      "loss": 0.0,
      "step": 2380
    },
    {
      "epoch": 2124.44,
      "grad_norm": NaN,
      "learning_rate": 0.015223044608921786,
      "loss": 0.0,
      "step": 2390
    },
    {
      "epoch": 2133.33,
      "grad_norm": NaN,
      "learning_rate": 0.015203040608121625,
      "loss": 0.0,
      "step": 2400
    },
    {
      "epoch": 2142.22,
      "grad_norm": NaN,
      "learning_rate": 0.015183036607321466,
      "loss": 0.0,
      "step": 2410
    },
    {
      "epoch": 2151.11,
      "grad_norm": NaN,
      "learning_rate": 0.015163032606521305,
      "loss": 0.0,
      "step": 2420
    },
    {
      "epoch": 2160.0,
      "grad_norm": NaN,
      "learning_rate": 0.015143028605721144,
      "loss": 0.0,
      "step": 2430
    },
    {
      "epoch": 2168.89,
      "grad_norm": NaN,
      "learning_rate": 0.015123024604920985,
      "loss": 0.0,
      "step": 2440
    },
    {
      "epoch": 2177.78,
      "grad_norm": NaN,
      "learning_rate": 0.015103020604120825,
      "loss": 0.0,
      "step": 2450
    },
    {
      "epoch": 2186.67,
      "grad_norm": NaN,
      "learning_rate": 0.015083016603320665,
      "loss": 0.0,
      "step": 2460
    },
    {
      "epoch": 2195.56,
      "grad_norm": NaN,
      "learning_rate": 0.015063012602520505,
      "loss": 0.0,
      "step": 2470
    },
    {
      "epoch": 2204.44,
      "grad_norm": NaN,
      "learning_rate": 0.015043008601720345,
      "loss": 0.0,
      "step": 2480
    },
    {
      "epoch": 2213.33,
      "grad_norm": NaN,
      "learning_rate": 0.015023004600920185,
      "loss": 0.0,
      "step": 2490
    },
    {
      "epoch": 2222.22,
      "grad_norm": NaN,
      "learning_rate": 0.015003000600120026,
      "loss": 0.0,
      "step": 2500
    },
    {
      "epoch": 2231.11,
      "grad_norm": NaN,
      "learning_rate": 0.014982996599319865,
      "loss": 0.0,
      "step": 2510
    },
    {
      "epoch": 2240.0,
      "grad_norm": NaN,
      "learning_rate": 0.014962992598519704,
      "loss": 0.0,
      "step": 2520
    },
    {
      "epoch": 2248.89,
      "grad_norm": NaN,
      "learning_rate": 0.014942988597719545,
      "loss": 0.0,
      "step": 2530
    },
    {
      "epoch": 2257.78,
      "grad_norm": NaN,
      "learning_rate": 0.014922984596919384,
      "loss": 0.0,
      "step": 2540
    },
    {
      "epoch": 2266.67,
      "grad_norm": NaN,
      "learning_rate": 0.014902980596119225,
      "loss": 0.0,
      "step": 2550
    },
    {
      "epoch": 2275.56,
      "grad_norm": NaN,
      "learning_rate": 0.014882976595319064,
      "loss": 0.0,
      "step": 2560
    },
    {
      "epoch": 2284.44,
      "grad_norm": NaN,
      "learning_rate": 0.014862972594518905,
      "loss": 0.0,
      "step": 2570
    },
    {
      "epoch": 2293.33,
      "grad_norm": NaN,
      "learning_rate": 0.014842968593718744,
      "loss": 0.0,
      "step": 2580
    },
    {
      "epoch": 2302.22,
      "grad_norm": NaN,
      "learning_rate": 0.014822964592918583,
      "loss": 0.0,
      "step": 2590
    },
    {
      "epoch": 2311.11,
      "grad_norm": NaN,
      "learning_rate": 0.014802960592118424,
      "loss": 0.0,
      "step": 2600
    },
    {
      "epoch": 2320.0,
      "grad_norm": NaN,
      "learning_rate": 0.014782956591318263,
      "loss": 0.0,
      "step": 2610
    },
    {
      "epoch": 2328.89,
      "grad_norm": NaN,
      "learning_rate": 0.014762952590518104,
      "loss": 0.0,
      "step": 2620
    },
    {
      "epoch": 2337.78,
      "grad_norm": NaN,
      "learning_rate": 0.014742948589717943,
      "loss": 0.0,
      "step": 2630
    },
    {
      "epoch": 2346.67,
      "grad_norm": NaN,
      "learning_rate": 0.014722944588917784,
      "loss": 0.0,
      "step": 2640
    },
    {
      "epoch": 2355.56,
      "grad_norm": NaN,
      "learning_rate": 0.014702940588117623,
      "loss": 0.0,
      "step": 2650
    },
    {
      "epoch": 2364.44,
      "grad_norm": NaN,
      "learning_rate": 0.014682936587317464,
      "loss": 0.0,
      "step": 2660
    },
    {
      "epoch": 2373.33,
      "grad_norm": NaN,
      "learning_rate": 0.014662932586517304,
      "loss": 0.0,
      "step": 2670
    },
    {
      "epoch": 2382.22,
      "grad_norm": NaN,
      "learning_rate": 0.014642928585717143,
      "loss": 0.0,
      "step": 2680
    },
    {
      "epoch": 2391.11,
      "grad_norm": NaN,
      "learning_rate": 0.014622924584916984,
      "loss": 0.0,
      "step": 2690
    },
    {
      "epoch": 2400.0,
      "grad_norm": NaN,
      "learning_rate": 0.014602920584116823,
      "loss": 0.0,
      "step": 2700
    },
    {
      "epoch": 2408.89,
      "grad_norm": NaN,
      "learning_rate": 0.014582916583316664,
      "loss": 0.0,
      "step": 2710
    },
    {
      "epoch": 2417.78,
      "grad_norm": NaN,
      "learning_rate": 0.014562912582516503,
      "loss": 0.0,
      "step": 2720
    },
    {
      "epoch": 2426.67,
      "grad_norm": NaN,
      "learning_rate": 0.014542908581716344,
      "loss": 0.0,
      "step": 2730
    },
    {
      "epoch": 2435.56,
      "grad_norm": NaN,
      "learning_rate": 0.014522904580916183,
      "loss": 0.0,
      "step": 2740
    },
    {
      "epoch": 2444.44,
      "grad_norm": NaN,
      "learning_rate": 0.014502900580116024,
      "loss": 0.0,
      "step": 2750
    },
    {
      "epoch": 2453.33,
      "grad_norm": NaN,
      "learning_rate": 0.014482896579315863,
      "loss": 0.0,
      "step": 2760
    },
    {
      "epoch": 2462.22,
      "grad_norm": NaN,
      "learning_rate": 0.014462892578515702,
      "loss": 0.0,
      "step": 2770
    },
    {
      "epoch": 2471.11,
      "grad_norm": NaN,
      "learning_rate": 0.014442888577715543,
      "loss": 0.0,
      "step": 2780
    },
    {
      "epoch": 2480.0,
      "grad_norm": NaN,
      "learning_rate": 0.014422884576915382,
      "loss": 0.0,
      "step": 2790
    },
    {
      "epoch": 2488.89,
      "grad_norm": NaN,
      "learning_rate": 0.014402880576115223,
      "loss": 0.0,
      "step": 2800
    },
    {
      "epoch": 2497.78,
      "grad_norm": NaN,
      "learning_rate": 0.014382876575315062,
      "loss": 0.0,
      "step": 2810
    },
    {
      "epoch": 2506.67,
      "grad_norm": NaN,
      "learning_rate": 0.014362872574514903,
      "loss": 0.0,
      "step": 2820
    },
    {
      "epoch": 2515.56,
      "grad_norm": NaN,
      "learning_rate": 0.014342868573714742,
      "loss": 0.0,
      "step": 2830
    },
    {
      "epoch": 2524.44,
      "grad_norm": NaN,
      "learning_rate": 0.014322864572914581,
      "loss": 0.0,
      "step": 2840
    },
    {
      "epoch": 2533.33,
      "grad_norm": NaN,
      "learning_rate": 0.014302860572114422,
      "loss": 0.0,
      "step": 2850
    },
    {
      "epoch": 2542.22,
      "grad_norm": NaN,
      "learning_rate": 0.014282856571314262,
      "loss": 0.0,
      "step": 2860
    },
    {
      "epoch": 2551.11,
      "grad_norm": NaN,
      "learning_rate": 0.014262852570514102,
      "loss": 0.0,
      "step": 2870
    },
    {
      "epoch": 2560.0,
      "grad_norm": NaN,
      "learning_rate": 0.014242848569713942,
      "loss": 0.0,
      "step": 2880
    },
    {
      "epoch": 2568.89,
      "grad_norm": NaN,
      "learning_rate": 0.014222844568913783,
      "loss": 0.0,
      "step": 2890
    },
    {
      "epoch": 2577.78,
      "grad_norm": NaN,
      "learning_rate": 0.014202840568113623,
      "loss": 0.0,
      "step": 2900
    },
    {
      "epoch": 2586.67,
      "grad_norm": NaN,
      "learning_rate": 0.014182836567313464,
      "loss": 0.0,
      "step": 2910
    },
    {
      "epoch": 2595.56,
      "grad_norm": NaN,
      "learning_rate": 0.014162832566513303,
      "loss": 0.0,
      "step": 2920
    },
    {
      "epoch": 2604.44,
      "grad_norm": NaN,
      "learning_rate": 0.014142828565713144,
      "loss": 0.0,
      "step": 2930
    },
    {
      "epoch": 2613.33,
      "grad_norm": NaN,
      "learning_rate": 0.014122824564912984,
      "loss": 0.0,
      "step": 2940
    },
    {
      "epoch": 2622.22,
      "grad_norm": NaN,
      "learning_rate": 0.014102820564112824,
      "loss": 0.0,
      "step": 2950
    },
    {
      "epoch": 2631.11,
      "grad_norm": NaN,
      "learning_rate": 0.014082816563312664,
      "loss": 0.0,
      "step": 2960
    },
    {
      "epoch": 2640.0,
      "grad_norm": NaN,
      "learning_rate": 0.014062812562512503,
      "loss": 0.0,
      "step": 2970
    },
    {
      "epoch": 2648.89,
      "grad_norm": NaN,
      "learning_rate": 0.014042808561712344,
      "loss": 0.0,
      "step": 2980
    },
    {
      "epoch": 2657.78,
      "grad_norm": NaN,
      "learning_rate": 0.014022804560912183,
      "loss": 0.0,
      "step": 2990
    },
    {
      "epoch": 2666.67,
      "grad_norm": NaN,
      "learning_rate": 0.014002800560112024,
      "loss": 0.0,
      "step": 3000
    },
    {
      "epoch": 2675.56,
      "grad_norm": NaN,
      "learning_rate": 0.013982796559311863,
      "loss": 0.0,
      "step": 3010
    },
    {
      "epoch": 2684.44,
      "grad_norm": NaN,
      "learning_rate": 0.013962792558511704,
      "loss": 0.0,
      "step": 3020
    },
    {
      "epoch": 2693.33,
      "grad_norm": NaN,
      "learning_rate": 0.013942788557711543,
      "loss": 0.0,
      "step": 3030
    },
    {
      "epoch": 2702.22,
      "grad_norm": NaN,
      "learning_rate": 0.013922784556911384,
      "loss": 0.0,
      "step": 3040
    },
    {
      "epoch": 2711.11,
      "grad_norm": NaN,
      "learning_rate": 0.013902780556111223,
      "loss": 0.0,
      "step": 3050
    },
    {
      "epoch": 2720.0,
      "grad_norm": NaN,
      "learning_rate": 0.013882776555311062,
      "loss": 0.0,
      "step": 3060
    },
    {
      "epoch": 2728.89,
      "grad_norm": NaN,
      "learning_rate": 0.013862772554510903,
      "loss": 0.0,
      "step": 3070
    },
    {
      "epoch": 2737.78,
      "grad_norm": NaN,
      "learning_rate": 0.013842768553710742,
      "loss": 0.0,
      "step": 3080
    },
    {
      "epoch": 2746.67,
      "grad_norm": NaN,
      "learning_rate": 0.013822764552910583,
      "loss": 0.0,
      "step": 3090
    },
    {
      "epoch": 2755.56,
      "grad_norm": NaN,
      "learning_rate": 0.013802760552110422,
      "loss": 0.0,
      "step": 3100
    },
    {
      "epoch": 2764.44,
      "grad_norm": NaN,
      "learning_rate": 0.013782756551310263,
      "loss": 0.0,
      "step": 3110
    },
    {
      "epoch": 2773.33,
      "grad_norm": NaN,
      "learning_rate": 0.013762752550510102,
      "loss": 0.0,
      "step": 3120
    },
    {
      "epoch": 2782.22,
      "grad_norm": NaN,
      "learning_rate": 0.013742748549709943,
      "loss": 0.0,
      "step": 3130
    },
    {
      "epoch": 2791.11,
      "grad_norm": NaN,
      "learning_rate": 0.013722744548909782,
      "loss": 0.0,
      "step": 3140
    },
    {
      "epoch": 2800.0,
      "grad_norm": NaN,
      "learning_rate": 0.013702740548109622,
      "loss": 0.0,
      "step": 3150
    },
    {
      "epoch": 2808.89,
      "grad_norm": NaN,
      "learning_rate": 0.013682736547309463,
      "loss": 0.0,
      "step": 3160
    },
    {
      "epoch": 2817.78,
      "grad_norm": NaN,
      "learning_rate": 0.013662732546509302,
      "loss": 0.0,
      "step": 3170
    },
    {
      "epoch": 2826.67,
      "grad_norm": NaN,
      "learning_rate": 0.013642728545709143,
      "loss": 0.0,
      "step": 3180
    },
    {
      "epoch": 2835.56,
      "grad_norm": NaN,
      "learning_rate": 0.013622724544908982,
      "loss": 0.0,
      "step": 3190
    },
    {
      "epoch": 2844.44,
      "grad_norm": NaN,
      "learning_rate": 0.013602720544108823,
      "loss": 0.0,
      "step": 3200
    },
    {
      "epoch": 2853.33,
      "grad_norm": NaN,
      "learning_rate": 0.013582716543308662,
      "loss": 0.0,
      "step": 3210
    },
    {
      "epoch": 2862.22,
      "grad_norm": NaN,
      "learning_rate": 0.013562712542508501,
      "loss": 0.0,
      "step": 3220
    },
    {
      "epoch": 2871.11,
      "grad_norm": NaN,
      "learning_rate": 0.013542708541708342,
      "loss": 0.0,
      "step": 3230
    },
    {
      "epoch": 2880.0,
      "grad_norm": NaN,
      "learning_rate": 0.013522704540908181,
      "loss": 0.0,
      "step": 3240
    },
    {
      "epoch": 2888.89,
      "grad_norm": NaN,
      "learning_rate": 0.013502700540108022,
      "loss": 0.0,
      "step": 3250
    },
    {
      "epoch": 2897.78,
      "grad_norm": NaN,
      "learning_rate": 0.013482696539307861,
      "loss": 0.0,
      "step": 3260
    },
    {
      "epoch": 2906.67,
      "grad_norm": NaN,
      "learning_rate": 0.013462692538507702,
      "loss": 0.0,
      "step": 3270
    },
    {
      "epoch": 2915.56,
      "grad_norm": NaN,
      "learning_rate": 0.013442688537707541,
      "loss": 0.0,
      "step": 3280
    },
    {
      "epoch": 2924.44,
      "grad_norm": NaN,
      "learning_rate": 0.013422684536907382,
      "loss": 0.0,
      "step": 3290
    },
    {
      "epoch": 2933.33,
      "grad_norm": NaN,
      "learning_rate": 0.013402680536107221,
      "loss": 0.0,
      "step": 3300
    },
    {
      "epoch": 2942.22,
      "grad_norm": NaN,
      "learning_rate": 0.01338267653530706,
      "loss": 0.0,
      "step": 3310
    },
    {
      "epoch": 2951.11,
      "grad_norm": NaN,
      "learning_rate": 0.013362672534506901,
      "loss": 0.0,
      "step": 3320
    },
    {
      "epoch": 2960.0,
      "grad_norm": NaN,
      "learning_rate": 0.01334266853370674,
      "loss": 0.0,
      "step": 3330
    },
    {
      "epoch": 2968.89,
      "grad_norm": NaN,
      "learning_rate": 0.013322664532906581,
      "loss": 0.0,
      "step": 3340
    },
    {
      "epoch": 2977.78,
      "grad_norm": NaN,
      "learning_rate": 0.01330266053210642,
      "loss": 0.0,
      "step": 3350
    },
    {
      "epoch": 2986.67,
      "grad_norm": NaN,
      "learning_rate": 0.013282656531306261,
      "loss": 0.0,
      "step": 3360
    },
    {
      "epoch": 2995.56,
      "grad_norm": NaN,
      "learning_rate": 0.0132626525305061,
      "loss": 0.0,
      "step": 3370
    },
    {
      "epoch": 3004.44,
      "grad_norm": NaN,
      "learning_rate": 0.013242648529705942,
      "loss": 0.0,
      "step": 3380
    },
    {
      "epoch": 3013.33,
      "grad_norm": NaN,
      "learning_rate": 0.01322264452890578,
      "loss": 0.0,
      "step": 3390
    },
    {
      "epoch": 3022.22,
      "grad_norm": NaN,
      "learning_rate": 0.01320264052810562,
      "loss": 0.0,
      "step": 3400
    },
    {
      "epoch": 3031.11,
      "grad_norm": NaN,
      "learning_rate": 0.01318263652730546,
      "loss": 0.0,
      "step": 3410
    },
    {
      "epoch": 3040.0,
      "grad_norm": NaN,
      "learning_rate": 0.0131626325265053,
      "loss": 0.0,
      "step": 3420
    },
    {
      "epoch": 3048.89,
      "grad_norm": NaN,
      "learning_rate": 0.01314262852570514,
      "loss": 0.0,
      "step": 3430
    },
    {
      "epoch": 3057.78,
      "grad_norm": NaN,
      "learning_rate": 0.01312262452490498,
      "loss": 0.0,
      "step": 3440
    },
    {
      "epoch": 3066.67,
      "grad_norm": NaN,
      "learning_rate": 0.013102620524104821,
      "loss": 0.0,
      "step": 3450
    },
    {
      "epoch": 3075.56,
      "grad_norm": NaN,
      "learning_rate": 0.01308261652330466,
      "loss": 0.0,
      "step": 3460
    },
    {
      "epoch": 3084.44,
      "grad_norm": NaN,
      "learning_rate": 0.013062612522504503,
      "loss": 0.0,
      "step": 3470
    },
    {
      "epoch": 3093.33,
      "grad_norm": NaN,
      "learning_rate": 0.013042608521704342,
      "loss": 0.0,
      "step": 3480
    },
    {
      "epoch": 3102.22,
      "grad_norm": NaN,
      "learning_rate": 0.013022604520904183,
      "loss": 0.0,
      "step": 3490
    },
    {
      "epoch": 3111.11,
      "grad_norm": NaN,
      "learning_rate": 0.013002600520104022,
      "loss": 0.0,
      "step": 3500
    },
    {
      "epoch": 3120.0,
      "grad_norm": NaN,
      "learning_rate": 0.012982596519303863,
      "loss": 0.0,
      "step": 3510
    },
    {
      "epoch": 3128.89,
      "grad_norm": NaN,
      "learning_rate": 0.012962592518503702,
      "loss": 0.0,
      "step": 3520
    },
    {
      "epoch": 3137.78,
      "grad_norm": NaN,
      "learning_rate": 0.012942588517703541,
      "loss": 0.0,
      "step": 3530
    },
    {
      "epoch": 3146.67,
      "grad_norm": NaN,
      "learning_rate": 0.012922584516903382,
      "loss": 0.0,
      "step": 3540
    },
    {
      "epoch": 3155.56,
      "grad_norm": NaN,
      "learning_rate": 0.012902580516103221,
      "loss": 0.0,
      "step": 3550
    },
    {
      "epoch": 3164.44,
      "grad_norm": NaN,
      "learning_rate": 0.012882576515303062,
      "loss": 0.0,
      "step": 3560
    },
    {
      "epoch": 3173.33,
      "grad_norm": NaN,
      "learning_rate": 0.012862572514502901,
      "loss": 0.0,
      "step": 3570
    },
    {
      "epoch": 3182.22,
      "grad_norm": NaN,
      "learning_rate": 0.012842568513702742,
      "loss": 0.0,
      "step": 3580
    },
    {
      "epoch": 3191.11,
      "grad_norm": NaN,
      "learning_rate": 0.012822564512902581,
      "loss": 0.0,
      "step": 3590
    },
    {
      "epoch": 3200.0,
      "grad_norm": NaN,
      "learning_rate": 0.01280256051210242,
      "loss": 0.0,
      "step": 3600
    },
    {
      "epoch": 3208.89,
      "grad_norm": NaN,
      "learning_rate": 0.012782556511302261,
      "loss": 0.0,
      "step": 3610
    },
    {
      "epoch": 3217.78,
      "grad_norm": NaN,
      "learning_rate": 0.0127625525105021,
      "loss": 0.0,
      "step": 3620
    },
    {
      "epoch": 3226.67,
      "grad_norm": NaN,
      "learning_rate": 0.012742548509701941,
      "loss": 0.0,
      "step": 3630
    },
    {
      "epoch": 3235.56,
      "grad_norm": NaN,
      "learning_rate": 0.01272254450890178,
      "loss": 0.0,
      "step": 3640
    },
    {
      "epoch": 3244.44,
      "grad_norm": NaN,
      "learning_rate": 0.012702540508101622,
      "loss": 0.0,
      "step": 3650
    },
    {
      "epoch": 3253.33,
      "grad_norm": NaN,
      "learning_rate": 0.01268253650730146,
      "loss": 0.0,
      "step": 3660
    },
    {
      "epoch": 3262.22,
      "grad_norm": NaN,
      "learning_rate": 0.012662532506501302,
      "loss": 0.0,
      "step": 3670
    },
    {
      "epoch": 3271.11,
      "grad_norm": NaN,
      "learning_rate": 0.01264252850570114,
      "loss": 0.0,
      "step": 3680
    },
    {
      "epoch": 3280.0,
      "grad_norm": NaN,
      "learning_rate": 0.01262252450490098,
      "loss": 0.0,
      "step": 3690
    },
    {
      "epoch": 3288.89,
      "grad_norm": NaN,
      "learning_rate": 0.01260252050410082,
      "loss": 0.0,
      "step": 3700
    },
    {
      "epoch": 3297.78,
      "grad_norm": NaN,
      "learning_rate": 0.01258251650330066,
      "loss": 0.0,
      "step": 3710
    },
    {
      "epoch": 3306.67,
      "grad_norm": NaN,
      "learning_rate": 0.012562512502500501,
      "loss": 0.0,
      "step": 3720
    },
    {
      "epoch": 3315.56,
      "grad_norm": NaN,
      "learning_rate": 0.01254250850170034,
      "loss": 0.0,
      "step": 3730
    },
    {
      "epoch": 3324.44,
      "grad_norm": NaN,
      "learning_rate": 0.012522504500900181,
      "loss": 0.0,
      "step": 3740
    },
    {
      "epoch": 3333.33,
      "grad_norm": NaN,
      "learning_rate": 0.01250250050010002,
      "loss": 0.0,
      "step": 3750
    },
    {
      "epoch": 3342.22,
      "grad_norm": NaN,
      "learning_rate": 0.012482496499299861,
      "loss": 0.0,
      "step": 3760
    },
    {
      "epoch": 3351.11,
      "grad_norm": NaN,
      "learning_rate": 0.0124624924984997,
      "loss": 0.0,
      "step": 3770
    },
    {
      "epoch": 3360.0,
      "grad_norm": NaN,
      "learning_rate": 0.01244248849769954,
      "loss": 0.0,
      "step": 3780
    },
    {
      "epoch": 3368.89,
      "grad_norm": NaN,
      "learning_rate": 0.01242248449689938,
      "loss": 0.0,
      "step": 3790
    },
    {
      "epoch": 3377.78,
      "grad_norm": NaN,
      "learning_rate": 0.01240248049609922,
      "loss": 0.0,
      "step": 3800
    },
    {
      "epoch": 3386.67,
      "grad_norm": NaN,
      "learning_rate": 0.01238247649529906,
      "loss": 0.0,
      "step": 3810
    },
    {
      "epoch": 3395.56,
      "grad_norm": NaN,
      "learning_rate": 0.0123624724944989,
      "loss": 0.0,
      "step": 3820
    },
    {
      "epoch": 3404.44,
      "grad_norm": NaN,
      "learning_rate": 0.01234246849369874,
      "loss": 0.0,
      "step": 3830
    },
    {
      "epoch": 3413.33,
      "grad_norm": NaN,
      "learning_rate": 0.01232246449289858,
      "loss": 0.0,
      "step": 3840
    },
    {
      "epoch": 3422.22,
      "grad_norm": NaN,
      "learning_rate": 0.012302460492098419,
      "loss": 0.0,
      "step": 3850
    },
    {
      "epoch": 3431.11,
      "grad_norm": NaN,
      "learning_rate": 0.01228245649129826,
      "loss": 0.0,
      "step": 3860
    },
    {
      "epoch": 3440.0,
      "grad_norm": NaN,
      "learning_rate": 0.012262452490498099,
      "loss": 0.0,
      "step": 3870
    },
    {
      "epoch": 3448.89,
      "grad_norm": NaN,
      "learning_rate": 0.01224244848969794,
      "loss": 0.0,
      "step": 3880
    },
    {
      "epoch": 3457.78,
      "grad_norm": NaN,
      "learning_rate": 0.012222444488897779,
      "loss": 0.0,
      "step": 3890
    },
    {
      "epoch": 3466.67,
      "grad_norm": NaN,
      "learning_rate": 0.01220244048809762,
      "loss": 0.0,
      "step": 3900
    },
    {
      "epoch": 3475.56,
      "grad_norm": NaN,
      "learning_rate": 0.012182436487297459,
      "loss": 0.0,
      "step": 3910
    },
    {
      "epoch": 3484.44,
      "grad_norm": NaN,
      "learning_rate": 0.0121624324864973,
      "loss": 0.0,
      "step": 3920
    },
    {
      "epoch": 3493.33,
      "grad_norm": NaN,
      "learning_rate": 0.012142428485697139,
      "loss": 0.0,
      "step": 3930
    },
    {
      "epoch": 3502.22,
      "grad_norm": NaN,
      "learning_rate": 0.012122424484896978,
      "loss": 0.0,
      "step": 3940
    },
    {
      "epoch": 3511.11,
      "grad_norm": NaN,
      "learning_rate": 0.012102420484096819,
      "loss": 0.0,
      "step": 3950
    },
    {
      "epoch": 3520.0,
      "grad_norm": NaN,
      "learning_rate": 0.012082416483296658,
      "loss": 0.0,
      "step": 3960
    },
    {
      "epoch": 3528.89,
      "grad_norm": NaN,
      "learning_rate": 0.0120624124824965,
      "loss": 0.0,
      "step": 3970
    },
    {
      "epoch": 3537.78,
      "grad_norm": NaN,
      "learning_rate": 0.012042408481696338,
      "loss": 0.0,
      "step": 3980
    },
    {
      "epoch": 3546.67,
      "grad_norm": NaN,
      "learning_rate": 0.01202240448089618,
      "loss": 0.0,
      "step": 3990
    },
    {
      "epoch": 3555.56,
      "grad_norm": NaN,
      "learning_rate": 0.012002400480096018,
      "loss": 0.0,
      "step": 4000
    },
    {
      "epoch": 3564.44,
      "grad_norm": NaN,
      "learning_rate": 0.01198239647929586,
      "loss": 0.0,
      "step": 4010
    },
    {
      "epoch": 3573.33,
      "grad_norm": NaN,
      "learning_rate": 0.011962392478495698,
      "loss": 0.0,
      "step": 4020
    },
    {
      "epoch": 3582.22,
      "grad_norm": NaN,
      "learning_rate": 0.011942388477695538,
      "loss": 0.0,
      "step": 4030
    },
    {
      "epoch": 3591.11,
      "grad_norm": NaN,
      "learning_rate": 0.01192238447689538,
      "loss": 0.0,
      "step": 4040
    },
    {
      "epoch": 3600.0,
      "grad_norm": NaN,
      "learning_rate": 0.011902380476095221,
      "loss": 0.0,
      "step": 4050
    },
    {
      "epoch": 3608.89,
      "grad_norm": NaN,
      "learning_rate": 0.01188237647529506,
      "loss": 0.0,
      "step": 4060
    },
    {
      "epoch": 3617.78,
      "grad_norm": NaN,
      "learning_rate": 0.0118623724744949,
      "loss": 0.0,
      "step": 4070
    },
    {
      "epoch": 3626.67,
      "grad_norm": NaN,
      "learning_rate": 0.01184236847369474,
      "loss": 0.0,
      "step": 4080
    },
    {
      "epoch": 3635.56,
      "grad_norm": NaN,
      "learning_rate": 0.01182236447289458,
      "loss": 0.0,
      "step": 4090
    },
    {
      "epoch": 3644.44,
      "grad_norm": NaN,
      "learning_rate": 0.01180236047209442,
      "loss": 0.0,
      "step": 4100
    },
    {
      "epoch": 3653.33,
      "grad_norm": NaN,
      "learning_rate": 0.01178235647129426,
      "loss": 0.0,
      "step": 4110
    },
    {
      "epoch": 3662.22,
      "grad_norm": NaN,
      "learning_rate": 0.0117623524704941,
      "loss": 0.0,
      "step": 4120
    },
    {
      "epoch": 3671.11,
      "grad_norm": NaN,
      "learning_rate": 0.01174234846969394,
      "loss": 0.0,
      "step": 4130
    },
    {
      "epoch": 3680.0,
      "grad_norm": NaN,
      "learning_rate": 0.01172234446889378,
      "loss": 0.0,
      "step": 4140
    },
    {
      "epoch": 3688.89,
      "grad_norm": NaN,
      "learning_rate": 0.01170234046809362,
      "loss": 0.0,
      "step": 4150
    },
    {
      "epoch": 3697.78,
      "grad_norm": NaN,
      "learning_rate": 0.011682336467293459,
      "loss": 0.0,
      "step": 4160
    },
    {
      "epoch": 3706.67,
      "grad_norm": NaN,
      "learning_rate": 0.0116623324664933,
      "loss": 0.0,
      "step": 4170
    },
    {
      "epoch": 3715.56,
      "grad_norm": NaN,
      "learning_rate": 0.011642328465693139,
      "loss": 0.0,
      "step": 4180
    },
    {
      "epoch": 3724.44,
      "grad_norm": NaN,
      "learning_rate": 0.01162232446489298,
      "loss": 0.0,
      "step": 4190
    },
    {
      "epoch": 3733.33,
      "grad_norm": NaN,
      "learning_rate": 0.011602320464092819,
      "loss": 0.0,
      "step": 4200
    },
    {
      "epoch": 3742.22,
      "grad_norm": NaN,
      "learning_rate": 0.01158231646329266,
      "loss": 0.0,
      "step": 4210
    },
    {
      "epoch": 3751.11,
      "grad_norm": NaN,
      "learning_rate": 0.011562312462492499,
      "loss": 0.0,
      "step": 4220
    },
    {
      "epoch": 3760.0,
      "grad_norm": NaN,
      "learning_rate": 0.011542308461692338,
      "loss": 0.0,
      "step": 4230
    },
    {
      "epoch": 3768.89,
      "grad_norm": NaN,
      "learning_rate": 0.01152230446089218,
      "loss": 0.0,
      "step": 4240
    },
    {
      "epoch": 3777.78,
      "grad_norm": NaN,
      "learning_rate": 0.011502300460092018,
      "loss": 0.0,
      "step": 4250
    },
    {
      "epoch": 3786.67,
      "grad_norm": NaN,
      "learning_rate": 0.01148229645929186,
      "loss": 0.0,
      "step": 4260
    },
    {
      "epoch": 3795.56,
      "grad_norm": NaN,
      "learning_rate": 0.011462292458491698,
      "loss": 0.0,
      "step": 4270
    },
    {
      "epoch": 3804.44,
      "grad_norm": NaN,
      "learning_rate": 0.01144228845769154,
      "loss": 0.0,
      "step": 4280
    },
    {
      "epoch": 3813.33,
      "grad_norm": NaN,
      "learning_rate": 0.011422284456891378,
      "loss": 0.0,
      "step": 4290
    },
    {
      "epoch": 3822.22,
      "grad_norm": NaN,
      "learning_rate": 0.01140228045609122,
      "loss": 0.0,
      "step": 4300
    },
    {
      "epoch": 3831.11,
      "grad_norm": NaN,
      "learning_rate": 0.011382276455291059,
      "loss": 0.0,
      "step": 4310
    },
    {
      "epoch": 3840.0,
      "grad_norm": NaN,
      "learning_rate": 0.011362272454490898,
      "loss": 0.0,
      "step": 4320
    },
    {
      "epoch": 3848.89,
      "grad_norm": NaN,
      "learning_rate": 0.011342268453690739,
      "loss": 0.0,
      "step": 4330
    },
    {
      "epoch": 3857.78,
      "grad_norm": NaN,
      "learning_rate": 0.011322264452890578,
      "loss": 0.0,
      "step": 4340
    },
    {
      "epoch": 3866.67,
      "grad_norm": NaN,
      "learning_rate": 0.011302260452090419,
      "loss": 0.0,
      "step": 4350
    },
    {
      "epoch": 3875.56,
      "grad_norm": NaN,
      "learning_rate": 0.011282256451290258,
      "loss": 0.0,
      "step": 4360
    },
    {
      "epoch": 3884.44,
      "grad_norm": NaN,
      "learning_rate": 0.011262252450490099,
      "loss": 0.0,
      "step": 4370
    },
    {
      "epoch": 3893.33,
      "grad_norm": NaN,
      "learning_rate": 0.011242248449689938,
      "loss": 0.0,
      "step": 4380
    },
    {
      "epoch": 3902.22,
      "grad_norm": NaN,
      "learning_rate": 0.011222244448889777,
      "loss": 0.0,
      "step": 4390
    },
    {
      "epoch": 3911.11,
      "grad_norm": NaN,
      "learning_rate": 0.011202240448089618,
      "loss": 0.0,
      "step": 4400
    },
    {
      "epoch": 3920.0,
      "grad_norm": NaN,
      "learning_rate": 0.011182236447289457,
      "loss": 0.0,
      "step": 4410
    },
    {
      "epoch": 3928.89,
      "grad_norm": NaN,
      "learning_rate": 0.011162232446489298,
      "loss": 0.0,
      "step": 4420
    },
    {
      "epoch": 3937.78,
      "grad_norm": NaN,
      "learning_rate": 0.011142228445689137,
      "loss": 0.0,
      "step": 4430
    },
    {
      "epoch": 3946.67,
      "grad_norm": NaN,
      "learning_rate": 0.011122224444888978,
      "loss": 0.0,
      "step": 4440
    },
    {
      "epoch": 3955.56,
      "grad_norm": NaN,
      "learning_rate": 0.011102220444088817,
      "loss": 0.0,
      "step": 4450
    },
    {
      "epoch": 3964.44,
      "grad_norm": NaN,
      "learning_rate": 0.011082216443288658,
      "loss": 0.0,
      "step": 4460
    },
    {
      "epoch": 3973.33,
      "grad_norm": NaN,
      "learning_rate": 0.011062212442488497,
      "loss": 0.0,
      "step": 4470
    },
    {
      "epoch": 3982.22,
      "grad_norm": NaN,
      "learning_rate": 0.011042208441688336,
      "loss": 0.0,
      "step": 4480
    },
    {
      "epoch": 3991.11,
      "grad_norm": NaN,
      "learning_rate": 0.011022204440888177,
      "loss": 0.0,
      "step": 4490
    },
    {
      "epoch": 4000.0,
      "grad_norm": NaN,
      "learning_rate": 0.011002200440088017,
      "loss": 0.0,
      "step": 4500
    },
    {
      "epoch": 4008.89,
      "grad_norm": NaN,
      "learning_rate": 0.010982196439287857,
      "loss": 0.0,
      "step": 4510
    },
    {
      "epoch": 4017.78,
      "grad_norm": NaN,
      "learning_rate": 0.010962192438487697,
      "loss": 0.0,
      "step": 4520
    },
    {
      "epoch": 4026.67,
      "grad_norm": NaN,
      "learning_rate": 0.010942188437687538,
      "loss": 0.0,
      "step": 4530
    },
    {
      "epoch": 4035.56,
      "grad_norm": NaN,
      "learning_rate": 0.010922184436887377,
      "loss": 0.0,
      "step": 4540
    },
    {
      "epoch": 4044.44,
      "grad_norm": NaN,
      "learning_rate": 0.010902180436087218,
      "loss": 0.0,
      "step": 4550
    },
    {
      "epoch": 4053.33,
      "grad_norm": NaN,
      "learning_rate": 0.010882176435287057,
      "loss": 0.0,
      "step": 4560
    },
    {
      "epoch": 4062.22,
      "grad_norm": NaN,
      "learning_rate": 0.010862172434486896,
      "loss": 0.0,
      "step": 4570
    },
    {
      "epoch": 4071.11,
      "grad_norm": NaN,
      "learning_rate": 0.010842168433686737,
      "loss": 0.0,
      "step": 4580
    },
    {
      "epoch": 4080.0,
      "grad_norm": NaN,
      "learning_rate": 0.010822164432886576,
      "loss": 0.0,
      "step": 4590
    },
    {
      "epoch": 4088.89,
      "grad_norm": NaN,
      "learning_rate": 0.010802160432086417,
      "loss": 0.0,
      "step": 4600
    },
    {
      "epoch": 4097.78,
      "grad_norm": NaN,
      "learning_rate": 0.010782156431286258,
      "loss": 0.0,
      "step": 4610
    },
    {
      "epoch": 4106.67,
      "grad_norm": NaN,
      "learning_rate": 0.010762152430486099,
      "loss": 0.0,
      "step": 4620
    },
    {
      "epoch": 4115.56,
      "grad_norm": NaN,
      "learning_rate": 0.010742148429685938,
      "loss": 0.0,
      "step": 4630
    },
    {
      "epoch": 4124.44,
      "grad_norm": NaN,
      "learning_rate": 0.010722144428885779,
      "loss": 0.0,
      "step": 4640
    },
    {
      "epoch": 4133.33,
      "grad_norm": NaN,
      "learning_rate": 0.010702140428085618,
      "loss": 0.0,
      "step": 4650
    },
    {
      "epoch": 4142.22,
      "grad_norm": NaN,
      "learning_rate": 0.010682136427285459,
      "loss": 0.0,
      "step": 4660
    },
    {
      "epoch": 4151.11,
      "grad_norm": NaN,
      "learning_rate": 0.010662132426485298,
      "loss": 0.0,
      "step": 4670
    },
    {
      "epoch": 4160.0,
      "grad_norm": NaN,
      "learning_rate": 0.010642128425685139,
      "loss": 0.0,
      "step": 4680
    },
    {
      "epoch": 4168.89,
      "grad_norm": NaN,
      "learning_rate": 0.010622124424884978,
      "loss": 0.0,
      "step": 4690
    },
    {
      "epoch": 4177.78,
      "grad_norm": NaN,
      "learning_rate": 0.010602120424084817,
      "loss": 0.0,
      "step": 4700
    },
    {
      "epoch": 4186.67,
      "grad_norm": NaN,
      "learning_rate": 0.010582116423284658,
      "loss": 0.0,
      "step": 4710
    },
    {
      "epoch": 4195.56,
      "grad_norm": NaN,
      "learning_rate": 0.010562112422484497,
      "loss": 0.0,
      "step": 4720
    },
    {
      "epoch": 4204.44,
      "grad_norm": NaN,
      "learning_rate": 0.010542108421684338,
      "loss": 0.0,
      "step": 4730
    },
    {
      "epoch": 4213.33,
      "grad_norm": NaN,
      "learning_rate": 0.010522104420884177,
      "loss": 0.0,
      "step": 4740
    },
    {
      "epoch": 4222.22,
      "grad_norm": NaN,
      "learning_rate": 0.010502100420084018,
      "loss": 0.0,
      "step": 4750
    },
    {
      "epoch": 4231.11,
      "grad_norm": NaN,
      "learning_rate": 0.010482096419283857,
      "loss": 0.0,
      "step": 4760
    },
    {
      "epoch": 4240.0,
      "grad_norm": NaN,
      "learning_rate": 0.010462092418483697,
      "loss": 0.0,
      "step": 4770
    },
    {
      "epoch": 4248.89,
      "grad_norm": NaN,
      "learning_rate": 0.010442088417683537,
      "loss": 0.0,
      "step": 4780
    },
    {
      "epoch": 4257.78,
      "grad_norm": NaN,
      "learning_rate": 0.010422084416883377,
      "loss": 0.0,
      "step": 4790
    },
    {
      "epoch": 4266.67,
      "grad_norm": NaN,
      "learning_rate": 0.010402080416083218,
      "loss": 0.0,
      "step": 4800
    },
    {
      "epoch": 4275.56,
      "grad_norm": NaN,
      "learning_rate": 0.010382076415283057,
      "loss": 0.0,
      "step": 4810
    },
    {
      "epoch": 4284.44,
      "grad_norm": NaN,
      "learning_rate": 0.010362072414482898,
      "loss": 0.0,
      "step": 4820
    },
    {
      "epoch": 4293.33,
      "grad_norm": NaN,
      "learning_rate": 0.010342068413682737,
      "loss": 0.0,
      "step": 4830
    },
    {
      "epoch": 4302.22,
      "grad_norm": NaN,
      "learning_rate": 0.010322064412882578,
      "loss": 0.0,
      "step": 4840
    },
    {
      "epoch": 4311.11,
      "grad_norm": NaN,
      "learning_rate": 0.010302060412082417,
      "loss": 0.0,
      "step": 4850
    },
    {
      "epoch": 4320.0,
      "grad_norm": NaN,
      "learning_rate": 0.010282056411282256,
      "loss": 0.0,
      "step": 4860
    },
    {
      "epoch": 4328.89,
      "grad_norm": NaN,
      "learning_rate": 0.010262052410482097,
      "loss": 0.0,
      "step": 4870
    },
    {
      "epoch": 4337.78,
      "grad_norm": NaN,
      "learning_rate": 0.010242048409681936,
      "loss": 0.0,
      "step": 4880
    },
    {
      "epoch": 4346.67,
      "grad_norm": NaN,
      "learning_rate": 0.010222044408881777,
      "loss": 0.0,
      "step": 4890
    },
    {
      "epoch": 4355.56,
      "grad_norm": NaN,
      "learning_rate": 0.010202040408081616,
      "loss": 0.0,
      "step": 4900
    },
    {
      "epoch": 4364.44,
      "grad_norm": NaN,
      "learning_rate": 0.010182036407281457,
      "loss": 0.0,
      "step": 4910
    },
    {
      "epoch": 4373.33,
      "grad_norm": NaN,
      "learning_rate": 0.010162032406481296,
      "loss": 0.0,
      "step": 4920
    },
    {
      "epoch": 4382.22,
      "grad_norm": NaN,
      "learning_rate": 0.010142028405681137,
      "loss": 0.0,
      "step": 4930
    },
    {
      "epoch": 4391.11,
      "grad_norm": NaN,
      "learning_rate": 0.010122024404880976,
      "loss": 0.0,
      "step": 4940
    },
    {
      "epoch": 4400.0,
      "grad_norm": NaN,
      "learning_rate": 0.010102020404080815,
      "loss": 0.0,
      "step": 4950
    },
    {
      "epoch": 4408.89,
      "grad_norm": NaN,
      "learning_rate": 0.010082016403280656,
      "loss": 0.0,
      "step": 4960
    },
    {
      "epoch": 4417.78,
      "grad_norm": NaN,
      "learning_rate": 0.010062012402480495,
      "loss": 0.0,
      "step": 4970
    },
    {
      "epoch": 4426.67,
      "grad_norm": NaN,
      "learning_rate": 0.010042008401680336,
      "loss": 0.0,
      "step": 4980
    },
    {
      "epoch": 4435.56,
      "grad_norm": NaN,
      "learning_rate": 0.010022004400880176,
      "loss": 0.0,
      "step": 4990
    },
    {
      "epoch": 4444.44,
      "grad_norm": NaN,
      "learning_rate": 0.010002000400080016,
      "loss": 0.0,
      "step": 5000
    }
  ],
  "logging_steps": 10,
  "max_steps": 10000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10000,
  "save_steps": 1000,
  "total_flos": 1.223694483456e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
